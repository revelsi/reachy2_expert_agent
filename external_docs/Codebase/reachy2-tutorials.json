[
  {
    "page_content": "# Tutorial n\u00b03 - SDK & Pollen-Vision : Reachy the greengrocer",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 1,
      "section": "Tutorial n\u00b03 - SDK & Pollen-Vision : Reachy the greengrocer"
    }
  },
  {
    "page_content": "In this tutorial, we will learn how to do tasks with Reachy using :\n- the SDK client to make it move,\n- the module pollen-vision to use object detection in its environment. \n\nHere, we are going to ask Reachy to sort fruits on a table and to drop them in user-defined places, according to which fruit it is. \n\nWhat you will learn : \n- How to do object detection\n- How to switch from the image frame to the robot frame\n- How to make Reachy move according to what it sees",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 2,
      "section": "Tutorial n\u00b03 - SDK & Pollen-Vision : Reachy the greengrocer"
    }
  },
  {
    "page_content": "## 1. Prerequisites\n\nTo use the SDK client and pollen-vision, you first need to install them. If it is not the case, checkout the section below \ud83d\udc47\n<details>\n\n<summary>Install the Python library reachy2-sdk</summary>\n\nIn general, you'd better **work in a virtual environment**. You have 2 different ways to install the sdk : \n- by running the following command:\n\n<code>\npip install reachy2-sdk -e .\n</code>\n\n- from source by following the instructions on the [GitHub repository](https://github.com/pollen-robotics/reachy2-sdk)\n\n</details>\n\n<details>\n\n<summary>Install the Python library pollen-vision</summary>\n\nIn general, you'd better **work in a virtual environment**. You have 2 different ways to install the sdk : \n- by running the following command:\n\n<code>\npip install pollen-vision\n</code>\n\n- from source by following the instructions on the [GitHub repository](https://github.com/pollen-robotics/pollen-vision)\n\n</details>\n\nIf you have never used the **SDK client** before, don't forget to do the [Getting Started notebooks](https://github.com/pollen-robotics/reachy2-sdk/tree/develop/src/examples), that will help you understand all the basics you need to know about it ! And if you want to go further, you can also check the [SDK documentation](https://pollen-robotics.github.io/reachy2-sdk/reachy2_sdk.html). \n\nYou can also familiarize yourself with **pollen-vision** by making the [Vision Models notebooks](https://github.com/pollen-robotics/pollen-vision/tree/develop/examples/vision_models_examples). \n",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 3,
      "section": "1. Prerequisites"
    }
  },
  {
    "page_content": "## 2. Set up\n\n### 2.1. Material\n\nFor this tutorial, you have to set up your environment. \nSo you need :\n- Reachy (obviously)\n- a table, no higher than the robot's elbows\n- a white plate\n- a white bowl\n- apples\n- oranges\n\n\n### 2.2. Scene\n\n> Your Reachy must be at a sufficient **height** so that its outstretched arms do not touch the mobile base. \n\n> As usual, Reachy needs to be in a safe environment with enough place to move around, no one in reachable space and no obstacles. And always keep the emergency stop button nearby!\n\nTo set up your scene : \n\n1.    Switch Reachy on and place it facing the table but 20 cm back (*it will need to bent its arms without obstacle, then it will be moved forward to the edge of the table*). \n2.    Make sure the torso camera is connected to the bedrock, as that's the one we'll be using here (*it's the one pluged on the outermost USB port*).\n3.    On the table, place the white plate on the left, then the oranges, the apples and the white bowl, such as the image below. Don't put them too far, Reachy doesn't have expandable arms.\n4.    You will need good lighting so that objects can be detected.\n\n<p align=\"center\"> \n    <img src=\"images/set_up_tuto_profil.jpg\" alt=\"Image 1\" style=\"width: 22.5%; margin-right: 10px;\"/>\n    <img src=\"images/set_up_tuto_face.jpg\" alt=\"Image 2\" style=\"width: 40%;\"/>\n</p>\n\n",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 4,
      "section": "2. Set up"
    }
  },
  {
    "page_content": "When you're all set up, you can move on to the next step.",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 5,
      "section": "2. Set up"
    }
  },
  {
    "page_content": "## 3. Preview\n\nIn this tutorial, we'll gradually build the program that will enable Reachy to sort fruit using the SDK Client and Pollen-Vision. This is what it your Reachy will be able to do at the end ! \n\n<p align = \"center\"> \n    <img src=\"images/gif_oranges.gif\" alt=\"Gif 1\" style=\"width: 27.5%; margin-right: 10px;\"/>\n    <img src=\"images/gif_apple_human.gif\" alt=\"Gif 2\" style=\"width: 40%; \"/>\n</p>\n\nExciting, isn't it ? So let's get to work !",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 6,
      "section": "3. Preview"
    }
  },
  {
    "page_content": "## 4. Let's build it ! \n\n### 4.1. Instanciation of the SDK",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 7,
      "section": "4. Let's build it !"
    }
  },
  {
    "page_content": "First, we will connect to the robot. To do so, we need to import the SDK client package and to instanciate a ReachySDK. That module is the one allowing us to control Reachy.\n\nTwo requirements :\n- Your computer needs to be on the same network as the robot.\n- You need to know your Reachy's IP : to do so, you have two options : \n    - you can check it on the dashboard (*.local_IP_address_of_your_robot:8000* in a browser), section Network. \n    - you can have a look at the small screen on Reachy's back, that will show you one at a time its Ethernet and its Wifi IP. \n\nNow, let's connect to it. ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 8,
      "section": "4. Let's build it !"
    }
  },
  {
    "page_content": "#import the package\nfrom reachy2_sdk import ReachySDK \n\n#connect to the robot\nreachy = ReachySDK('localhost') # replace 'localhost' with the actual IP address of your Reachy\nprint(\"Reachy is connected :\", reachy.is_connected())",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 9,
      "section": "4. Let's build it !"
    }
  },
  {
    "page_content": "If you are getting the message \"Could not connect to Reachy\", make sure that :\n-  Reachy is turned on\n- the reachy2_core.service is running \n- the IP address is correct. \n\n*More info on the debug section of the [sdk documentation](https://pollen-robotics.github.io/reachy2-docs/help/help/recovering/).<br>*",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 10,
      "section": "4. Let's build it !"
    }
  },
  {
    "page_content": "### 4.2. Instanciation of Pollen-Vision \n\nThen, we will use Pollen-Vision repository to be able to detect objects from the camera view thanks to AI and extract informations about them, for Reachy to adapt its actions to its environment.\n\nMore specifically, we'll be using the *Perception* module, which combines object detection using a [YOLO model](https://docs.ultralytics.com/models/yolo-world/), image segmentation and conversion of the extracted informations from pixels into real-world coordinates. \n",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 11,
      "section": "4.2. Instanciation of Pollen-Vision"
    }
  },
  {
    "page_content": "#### 4.2.1. How does it work\n\nWe set the name of the objects we're interested in, and the model detects those objects with a prediction confidence score (from 0, the lowest, to 1, the highest). And from those detections, we get different informations about the objects such as the coordinates of the bounding box, the centroid, and the depth. As this is done on Reachy's camera view, we can track the objects dynamically and minimize the impact of artifacts by retaining only the objects that are always detected from one frame to the next. \n\nThen, we can switch from pixels frame to Reachy's frame and get the informations of the objects in the real world frame, using parameters such as the camera intrinsic matrix (that allows to transform pixel coordinates into 3D coordinates from the camera frame) and the transform of the camera in Reachy'sframe (*T_reachy_cam*, which is known and allows to switch from the camera to Reachy's frame of reference). \n",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 12,
      "section": "4.2.1. How does it work"
    }
  },
  {
    "page_content": "#### 4.2.2. How to instanciate it\n\nTo define it, we need to get all the informations we talked about before : \n\n- the intrinsic matrix of the camera, that can be found thanks to the PollenSDKCameraWrapper, a class of pollen-vision that allows to get informations about Reachy's cameras. \n- the T_reachy_cam\n- the prediction score threshold for the objects to be considered\n- the frequency at which we want to perform object detection on the camera frames",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 13,
      "section": "4.2.2. How to instanciate it"
    }
  },
  {
    "page_content": "##### **PollenSDKCameraWrapper**\n\nWe are going to use this module to get informations about Reachy's camera, such as : \n- the images captured by the camera, with *get_data()* \n- parameters like the camera intrinsic matrix (named K by convention) with *get_K()* for the RGB camera, and *get_depth_K()* for the depth camera. \n\nWe need to instanciate it, with the SDK client object we instanciated earlier. Then, we are going to print the image captured by the torso camera when you execute the cell. ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 14,
      "section": "**PollenSDKCameraWrapper**"
    }
  },
  {
    "page_content": "from pollen_vision.camera_wrappers.pollen_sdk_camera.pollen_sdk_camera_wrapper import PollenSDKCameraWrapper\n\n# instanciation of the camera wrapper\nr_cam = PollenSDKCameraWrapper(reachy)\n\n# get the image as a np.array, and the timestamp of the image\ndata, _, timestamp = r_cam.get_data()\n\n# displays the image from the RGB camera (depth camera is available by changing 'left' to 'depth')\nfrom PIL import Image\nimg = data['left']\nImage.fromarray(img) ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 15,
      "section": "**PollenSDKCameraWrapper**"
    }
  },
  {
    "page_content": "##### **T_reachy_cam**\n\nNow, we need to define the T_reachy_cam. The camera frame is oriented differently from the robot frame, with the x axis to the left, the y axis down and the z axis back (as a reminder, the robot frame of reference has the x axis forward, the y axis to the left and the z axis up). Moreover, the torso camera is tilted downwards. ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 16,
      "section": "**T_reachy_cam**"
    }
  },
  {
    "page_content": "from reachy2_sdk.utils.utils import invert_affine_transformation_matrix\n\nT_cam_reachy = reachy.cameras.depth.get_extrinsics()\nT_reachy_cam = invert_affine_transformation_matrix(T_cam_reachy)",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 17,
      "section": "**T_reachy_cam**"
    }
  },
  {
    "page_content": "##### **Labels** \n\nTo know what to use as a frequency and prediction score threshold, you first need to set the labels you're gonna use to say what you want to detect. Indeed, to use IA model for object detection, it's important to find the most relevant label for the model to find the objects, and it's not always the most intuitive words. \n\nTo test those, you can use directly two submodules used in Perception to see if your objects are correctly detected on your image :\n-  the YOLO model (*the one that detects objects*) \n-  the Annotator (*the one that allows the visualization*).\n\nBe creative, try even non-instinctive words, with more or less precise adjectives, and test everything you can ! \n\nIn this tutorial, we decide to define two objects in which to place the fruit: a right-hand side and a left-hand side. And we'll define the type of fruit to be placed on the right and the one on the left. Here, we have a plate on the left and a bowl on the right, and the fruits to be placed are oranges and apples.\n\nWe are going to try classic labels for the detection, in the candidate_labels, but you'll see that we need to adjust those because the detection won't be good enough. \n\n*Don't worry if the instanciation of the YoloWorldWrapper is taking a while, it can take about 1-1.5min to create it the first time. And you can have a warning message, with no impact on the rest of the tutorial*.",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 18,
      "section": "**Labels**"
    }
  },
  {
    "page_content": "from pollen_vision.vision_models.object_detection import YoloWorldWrapper\nfrom pollen_vision.utils import get_bboxes\nfrom pollen_vision.utils import Annotator\n\n#those first labels won't be good enough\nlabels = [\"orange\", \"apple\", \"plate\", \"bowl\"]\n\nyolo = YoloWorldWrapper() #allows to use the YOLO model for object detection\nannotator = Annotator() #allows to get annotated image with the detected objects\nyolo_predictions = yolo.infer(im=img, candidate_labels=labels, detection_threshold=0.15) \nbboxes = get_bboxes(yolo_predictions) #returns the bounding boxes of the detected objects\nimg_annotated = annotator.annotate(im=img, detection_predictions=yolo_predictions) \nImage.fromarray(img_annotated)\n",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 19,
      "section": "**Labels**"
    }
  },
  {
    "page_content": "Here, you could see that the oranges aren't detected as \"orange\" and may be detected as apples. We are going to try new labels to be more effective. Also, you can add adjectives to be sure not to have any false positive. ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 20,
      "section": "**Labels**"
    }
  },
  {
    "page_content": "labels = [\"orange ball\", \"apple\", \"white plate\", \"white bowl\"]\nyolo_predictions = yolo.infer(im=img, candidate_labels=labels, detection_threshold=0.15)\nbboxes = get_bboxes(yolo_predictions)\nimg_annotated = annotator.annotate(im=img, detection_predictions=yolo_predictions)\nImage.fromarray(img_annotated)",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 21,
      "section": "**Labels**"
    }
  },
  {
    "page_content": "Now, we have good labels to detect our objects. \n\n\n##### **Detection threshold & Frequency**\n\nNow that we have our labels, we can see that we also have the prediction scores on the image : that will help you set the detection threshold when you instanciate your Perception object. Here, we decide to set it to 0.2, because the oranges are detected with a low score. \n\nThe frequency depends on how well your objects are detected (if they're not well detected, a higher frequency will increase the number of frames over which the model attempts detection). Here, we are going to set it to 40 because the oranges aren't detected often. ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 22,
      "section": "**Labels**"
    }
  },
  {
    "page_content": "#### 4.2.3. Instanciation\n\nNow we have all the elements to instanciate the Perception object : the CameraWrapper, the T_reachy_cam, the frequency of detection and the detection threshold.",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 23,
      "section": "4.2.3. Instanciation"
    }
  },
  {
    "page_content": "from pollen_vision.perception import Perception\n\nperception = Perception(r_cam, T_reachy_cam, freq=40, yolo_thres=0.2)\n\nprint(\"The Perception object is created.\")",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 24,
      "section": "4.2.3. Instanciation"
    }
  },
  {
    "page_content": "And now, we can start the objects detection and the tracking: we start by setting the tracked objects, then we launch it. \n\nYou can display the image from the camera continuously, together with the objects detected, their bounding boxes and their masks, the prediction scores and their centroids, thanks to the \"visualize=True\".",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 25,
      "section": "4.2.3. Instanciation"
    }
  },
  {
    "page_content": "left_obj_target, right_obj_target = \"white plate\", \"white bowl\"\nobj_to_left, obj_to_right = \"orange ball\", \"apple\"\n\nperception.set_tracked_objects([obj_to_left, obj_to_right, left_obj_target, right_obj_target])\nperception.start(visualize=True)",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 26,
      "section": "4.2.3. Instanciation"
    }
  },
  {
    "page_content": "You have now a window with the camera view and the detected objects. The Perception part is now on.\n\n> *If you didn't get the new window, the notebook crashed unfortunately. Please restart your kernel and execute again the cells above.*",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 27,
      "section": "4.2.3. Instanciation"
    }
  },
  {
    "page_content": "### 4.3. Set Reachy ready",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 28,
      "section": "4.3. Set Reachy ready"
    }
  },
  {
    "page_content": "We want Reachy to be in front of the table where there are the fruits, in a position that allows it to access the table without obstructing its camera's view. ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 29,
      "section": "4.3. Set Reachy ready"
    }
  },
  {
    "page_content": "Reachy has its motors off by default. So first, we need to turn them on. \n\n> Put Reachy in a safe environment, with no one in reachable space and no obstacles, and with someone who has the emergency stop button nearby. \n\nIf you have followed the set-up correctly, Reachy is normally positioned facing the table, 20 cm backwards. So now, we turn it on and put it on its default pose (head and arms straight).",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 30,
      "section": "4.3. Set Reachy ready"
    }
  },
  {
    "page_content": "reachy.turn_on()\nreachy.goto_posture('default')",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 31,
      "section": "4.3. Set Reachy ready"
    }
  },
  {
    "page_content": "Now, we can define the waiting pose we want it to take before starting grasping the fruits. \n\nFor that, we will define a function to set the joint values of every part of the robot. \n\nHere, we want the arms to be parallel to the ground, with the elbows back and slightly apart, grippers open. We want the head to look at its working area, pointing towards the central point 30cm in front of its end-effectors. We let the possibility to set the duration of the moves directly in the parameters of the function (by default, it's going to take 2 seconds). \n\n> You can set your own pose according to your environment, but be aware that there is no limitation to avoid collision between parts of the robot (right arm against left arm, arm against forearm, arm against torso, etc.). So it's your responsability to try the pose on a virtual set-up first or using the forward_kinematics() when you place the body parts where you will want them to be (the robot needs to be compliant for that : you'll have to turn it off).\n\nFor this tutorial, we will always define first the function, then execute it :",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 32,
      "section": "4.3. Set Reachy ready"
    }
  },
  {
    "page_content": "import time \n\ndef get_to_waiting_pose(reachy: ReachySDK, duration: float = 2) -> None :\n    r_arm_move = reachy.r_arm.goto([30,10,-15,-115,0,0,-15], duration)\n    l_arm_move = reachy.l_arm.goto([30,-10,15,-115,0,0,15], duration)\n\n    reachy.r_arm.gripper.open()\n    reachy.l_arm.gripper.open()\n\n    waiting_position = reachy.r_arm.forward_kinematics([30,10,-15,-115,0,0,-15])[:3,3]\n    head_move = reachy.head.look_at(waiting_position[0]+0.3, 0, waiting_position[2], duration, wait = True)\n\n    print(\"Reachy in waiting pose\")\n\nprint(\"Function get_to_waiting_pose defined.\")",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 33,
      "section": "4.3. Set Reachy ready"
    }
  },
  {
    "page_content": "Make the robot move now : ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 34,
      "section": "4.3. Set Reachy ready"
    }
  },
  {
    "page_content": "get_to_waiting_pose(reachy, duration = 3) #each move will take 3 seconds to be done, all in the same time. \n",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 35,
      "section": "4.3. Set Reachy ready"
    }
  },
  {
    "page_content": "Now we are going to move Reachy forward to the edge of the table, by controlling the mobile base. We can either :\n-  **use the *goto* method**: we ask it to move 0.2m forward from its reference frame. \n\nBe aware that this frame is set at the start-up of the robot (so when you press the start button on the mobile base station), and not when you connect to the robot. Be careful, the reference frame doesn't update, unless you call yourself the function *reachy.mobile.reset_odometry()*. That means that if you want to move again 0.2m forward after the first move, you will have to set the command with x = 0.2 + 0.2 = 0.4m. So, to be sure the reference frame is well set, we can call the reset_odometry before sending the command. \n\n- **use the *translate_by* method** : we ask it to move 0.2m forward from its actual position. \n\nThis second method seems safer in our case, so you can make the robot move now : ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 36,
      "section": "4.3. Set Reachy ready"
    }
  },
  {
    "page_content": "reachy.mobile_base.reset_odometry()\nreachy.mobile_base.translate_by(x= 0.2, y=0.0)",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 37,
      "section": "4.3. Set Reachy ready"
    }
  },
  {
    "page_content": "If Reachy is not close enough to the table, you can adjust yourself by calling again the *translate_by* method, or turn it off (<code>reachy.mobile_base.turn_off()</code>) and move the robot yourself to the right place. \n\nNow, it's set for its grasping task. Let's see how we can deal with it !",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 38,
      "section": "4.3. Set Reachy ready"
    }
  },
  {
    "page_content": "### 4.4. Construct our program\n\nWe need to construct the program that will make Reachy sort the fruits. \n\nFor that, we need it to:  \n1.    Detect the specific objects\n3.    Determine the closest object to the corresponding effector (left one for the oranges, right one for the apples)\n4.    Make the corresponding arm move to the closest fruit et grasp it\n5.    Make it go above the target object (the plate for the oranges, the bowl for the apples) and drop it\n6.    Go back to its waiting pose. \n\nSo, let's do that step by step !",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 39,
      "section": "4.4. Construct our program"
    }
  },
  {
    "page_content": "#### 4.4.1. Detect the specific objects\n\nWe need to use the Perception module to detect the oranges, the apples, the plate and the bowl, and to extract pertinent informations.\n\nif you have followed the set-up correctly, you should have a white plate and oranges on Reachy's left side and a white bowl and apples on its right side, like in the photos below. ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 40,
      "section": "4.4.1. Detect the specific objects"
    }
  },
  {
    "page_content": "<p align=\"center\"> \n    <img src=\"images/Reachy_fruit_face.jpg\" alt=\"Image face\" style=\"width: 20%; ; margin-right: 10px;\"/>\n    <img src=\"images/Reachy_fruits_above.jpg\" alt=\"Image above\" style=\"width: 20%;\"/>\n</p>",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 41,
      "section": "4.4.1. Detect the specific objects"
    }
  },
  {
    "page_content": "Check on the camera view displayer that all your objects are detected and not hidden by the arms of the robot. If so, move them a bit. Fruits need to be standing up to be well detected. \n\n\nNow, you can use the function *get_objects_infos()* of Perception, to get all the informations of the extracted objects who have a prediction score above the detection threshold you set.",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 42,
      "section": "4.4.1. Detect the specific objects"
    }
  },
  {
    "page_content": "detection_threshold = 0.2\ndetected_objects = perception.get_objects_infos(detection_threshold)\nprint(detected_objects)",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 43,
      "section": "4.4.1. Detect the specific objects"
    }
  },
  {
    "page_content": "As you can see, it returns a list of unsorted dictionnaries representing each object detected among all the researched objects, with different informations, as the label, the score prediction, the bounding box, the mask... ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 44,
      "section": "4.4.1. Detect the specific objects"
    }
  },
  {
    "page_content": "So we need to differenciate each object, depending on its type. We will define a function to do that. \n\nWe give as parameters the Perception instanciation, the type of fruits to be placed to the left side, to the right side, the left target object, the right target object and the detection threshold. And it will return the list of dictionnaries of oranges, of apples, and the dictionnaries of the white plate and the white bowl. \n\nTo avoid errors, we make sure to detect the two target objects to continue : if there are not detected, we reduce the detection threshold until a minimum threshold. ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 45,
      "section": "4.4.1. Detect the specific objects"
    }
  },
  {
    "page_content": "from typing import Tuple, Union, List, Dict, Optional\n\ndef get_selected_objects(\n    perception: Perception, \n    obj_to_left: str, \n    obj_to_right: str, \n    left_obj_target: str, \n    right_obj_target: str, \n    detection_threshold: float\n) -> Tuple[List[dict], List[dict], dict, dict]:\n    \n    detected_objects = perception.get_objects_infos(detection_threshold)\n\n    # extracting the fruits to be drop to the left and to the right side\n    objects_to_left = [obj for obj in detected_objects if obj['name'] == obj_to_left]\n    objects_to_right = [obj for obj in detected_objects if obj['name'] == obj_to_right]\n\n    # Reduction of the detection threshold while the containers aren't detected until a minimum threshold is reached\n    min_detection_threshold = 0.1\n    while True : \n        left_target = [obj for obj in detected_objects if obj['name'] == left_obj_target]\n        right_target = [obj for obj in detected_objects if obj['name'] == right_obj_target]\n\n        if left_target != [] and right_target != [] :\n            break\n        \n        if detection_threshold > min_detection_threshold : \n            detection_threshold -= 0.01\n            print(f\"Targets not found, lowering threshold to {detection_threshold}\")\n        else : \n            print(\"Targets not found, new try with the minimal threshold\")\n        detected_objects = perception.get_objects_infos(detection_threshold)\n        time.sleep(1.5)\n        \n\n    print(f'{len(detected_objects)} objects detected including {len(objects_to_left)} {obj_to_left}s, {len(objects_to_right)} {obj_to_right}s, {len(left_target)} {left_obj_target} and {len(right_target)} {right_obj_target}.')\n    \n    return objects_to_left, objects_to_right, left_target[0], right_target[0]\n\nprint(\"Function get_selected_objects defined.\")",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 46,
      "section": "4.4.1. Detect the specific objects"
    }
  },
  {
    "page_content": "Now, you can execute this function to have the list of dictionnaries of each object type. ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 47,
      "section": "4.4.1. Detect the specific objects"
    }
  },
  {
    "page_content": "oranges, apples, plate, bowl = get_selected_objects(perception, obj_to_left, obj_to_right, left_obj_target, right_obj_target, detection_threshold)",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 48,
      "section": "4.4.1. Detect the specific objects"
    }
  },
  {
    "page_content": "#### 4.4.2. Get the closest fruit to grasp\n\nNow we want to determine the fruit that requires the less effort from Reachy to grasp, meaning the fruit that is the closest to his corresponding effector (oranges will only be grasped by the left arm, and apples by the right arm). So we will define a function for that. \n\nWe can first define a function to calculate distance between centroids of two objects defined by a dictionnary.\n",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 49,
      "section": "4.4.2. Get the closest fruit to grasp"
    }
  },
  {
    "page_content": "import numpy.typing as npt\nimport numpy as np\n\ndef distance_between_objects(obj1: npt.NDArray[np.float64], obj2: npt.NDArray[np.float64]) -> float : \n    return np.linalg.norm(obj1[:3,3]-obj2[:3,3])\n\ndist = distance_between_objects(oranges[0]['pose'], plate['pose']) #returns the distance in meters between the first orange and the plate\nprint(f'Distance between the first orange and the plate: {dist:.2f}m.')",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 50,
      "section": "4.4.2. Get the closest fruit to grasp"
    }
  },
  {
    "page_content": "Then, we need to define functions to add some conditions for the object to be graspable : \n\n- it has not to be already in one of the target objects  : that's the goal of the *object_in_target()* function (it uses a threshold of distance between the centroids)\n\n- it has to be different from the last grasped object : that's the goal of the *is_a_new_object()* function, it avoids trying to grasp the same object a second time because of image capture latency and uses also a threshold of distance between centroids. \n\n- it has to be far enough from the robot to avoid collision with its torso : that's the goal of the *is_too_close* function.",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 51,
      "section": "4.4.2. Get the closest fruit to grasp"
    }
  },
  {
    "page_content": "def object_in_target(obj: Dict, target: Dict, threshold: float) -> bool:\n    return distance_between_objects(obj['pose'], target['pose']) < threshold\n\n\ndef is_a_new_object(actual_obj: Dict, former_obj: Dict, threshold: float) -> bool:\n    if former_obj == []: \n        return True\n    else: \n        return distance_between_objects(actual_obj['pose'], former_obj['pose']) > threshold\n\n\ndef is_too_close(obj: Dict, x_lim: float = 0.15, y_lim: float = 0.15) -> bool:\n    if obj['pose'][0,3] < x_lim and abs(obj['pose'][1,3]) < y_lim:\n        print(f\"Object {obj['name']} at {obj['pose'][:3,3]} is too close to the robot\")\n        return True\n    return False\n\nprint(\"Functions object_in_target, is_a_new_object and is_too_close defined.\")",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 52,
      "section": "4.4.2. Get the closest fruit to grasp"
    }
  },
  {
    "page_content": "Now, we can define our global function, that will test for each fruit the distance with its corresponding effector and the compliance to the conditions (done by an internal function). It will return the closest object as a dictionnary and the target side as a string. It will print the closest fruit and its position in Reachy's frame. If no object respects the conditions, it will return an empty list.",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 53,
      "section": "4.4.2. Get the closest fruit to grasp"
    }
  },
  {
    "page_content": "def get_closest_object(\n    reachy: ReachySDK,  \n    former_object : Dict, \n    obj_to_left: List[dict], \n    obj_to_right: List[dict], \n    left_target: Dict, \n    right_target: Dict, \n    dist_threshold : float\n) -> Tuple[Optional[Dict], Optional[str]]:\n\n    def find_closest(objects, effector_pos, side_name):\n        #the closest_object is an object that is not already in one of the targets, not too close to the robot and not the same as the former_object\n        nonlocal closest_object, min_dist, side\n        for obj in objects:\n            dist_with_effector = distance_between_objects(obj['pose'], effector_pos)\n            if (dist_with_effector < min_dist and \n                not object_in_target(obj, left_target, dist_threshold) and\n                not object_in_target(obj, right_target, dist_threshold) and \n                not is_too_close(obj) and \n                is_a_new_object(obj, former_object, fruit_radius)):\n                \n                closest_object = obj\n                min_dist = dist_with_effector\n                side = side_name\n\n    closest_object, side = [], None\n    min_dist = np.inf\n    fruit_radius = 0.02\n\n    #Get position of the end effectors\n    position_l_effector = reachy.l_arm.forward_kinematics()\n    position_r_effector = reachy.r_arm.forward_kinematics()\n    \n    #if the containers are detected, we look for the closest object with its relative effector\n    if left_target : \n        find_closest(obj_to_left, position_l_effector, 'left')\n\n    if right_target:\n        find_closest(obj_to_right, position_r_effector, 'right')\n\n    if closest_object:\n        print(f\"Closest fruit is {closest_object['name']} at the coordinates {closest_object['pose'][:3,3]}\")\n    else: \n        print(\"No fruit detected in the workspace\")\n\n    return closest_object, side\n\nprint(\"Function get_closest_object defined.\")",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 54,
      "section": "4.4.2. Get the closest fruit to grasp"
    }
  },
  {
    "page_content": "Now it's your turn to determine which fruit is the closest : ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 55,
      "section": "4.4.2. Get the closest fruit to grasp"
    }
  },
  {
    "page_content": "former_object = []\nclosest_object, target_side = get_closest_object(reachy, former_object, oranges, apples, plate, bowl, 0.15)",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 56,
      "section": "4.4.2. Get the closest fruit to grasp"
    }
  },
  {
    "page_content": "#### 4.4.3. Grasp the fruit\n\nNow that we know which is the closest fruit, we need to make Reachy catch it. For that, we have to follow some steps : ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 57,
      "section": "4.4.3. Grasp the fruit"
    }
  },
  {
    "page_content": "First, we need to determine the grasp pose for the corresponding effector from the fruit centroid.\n\n- For the position, we use offsets for each axis for the object to be in the middle of the gripper (the origin of the effector frame is located before the gripper)\n\n- For the rotation :\n    - we want the effector to be parallel to the ground, so the rotation in y axis is -90\u00b0 (0 is with effector pointing to the ground).\n    - we want the effector to orient towards the object. For that, we use the bent_position, meaning the position of the effectors when Reachy has its elbows facing torso, bent at 90\u00b0. We will use the angle between the line formed by the forearm in the bent_position and the line between the robot's elbow when facing the torso, and the object to be reached. \n\n    <p align=\"center\"><img src=\"images/angle.jpg\" alt=\"Image angle\" style=\"width: 20%; ; margin-right: 10px;\"/></p>\n\nThis function needs to get the dictionnary of the selected object and the target side, to return the matrix 4x4 of the grasp pose. ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 58,
      "section": "4.4.3. Grasp the fruit"
    }
  },
  {
    "page_content": "from reachy2_sdk.utils.utils import get_pose_matrix\n\ndef get_goal_pose(object_pose_dict: Dict, target_side: str) -> npt.NDArray[np.float64]: \n    #we define the effectors position when the robot has its forearms parallel to the ground\n    bent_position = np.array([0.38622, 0.22321, -0.27036]) if target_side == 'left' else np.array([0.38622, -0.22321, -0.27036])\n    \n    object_pose = np.copy(object_pose_dict['pose'])\n    \n    # for the goal position to be in the gripper center\n    object_pose [0,3] -= 0.04\n    object_pose [1,3] = object_pose [1,3] + 0.01 if target_side == 'left' else object_pose [1,3] - 0.01\n    object_pose [2,3] -= 0.04\n\n    #set the orientation of the effector to reach the object\n    dy = object_pose[1,3] - bent_position[1]\n    dx = object_pose[0,3]\n\n    angle_x_rad = np.arctan2(dx, dy)\n    angle_x = 90 - np.degrees(angle_x_rad)\n\n    target_pose = get_pose_matrix(object_pose[:3,3], [angle_x, -90, 0])\n\n    print(f\"goal position {target_pose[:3,3]}, goal rotation {[angle_x, -90, 0]}\\n\")\n    \n    return target_pose\n\nprint(\"Function get_goal_pose defined.\")",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 59,
      "section": "4.4.3. Grasp the fruit"
    }
  },
  {
    "page_content": "We are going to try it on our closest fruit to get the grasp pose needed to catch it : ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 60,
      "section": "4.4.3. Grasp the fruit"
    }
  },
  {
    "page_content": "grasp_pose = get_goal_pose(closest_object, target_side)\nprint(f'The grasp pose is  :\\n {grasp_pose}')",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 61,
      "section": "4.4.3. Grasp the fruit"
    }
  },
  {
    "page_content": "reachy.r_arm.forward_kinematics()",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 62,
      "section": "4.4.3. Grasp the fruit"
    }
  },
  {
    "page_content": "But if you try to go directly from the waiting pose to the grasp pose, you'll see that sometimes the trajectory may bump into the object, and the movement may need to pass through an intermediate point to reach the fruit optimally. \n\nHere, we are going to add a pregrasp pose, meaning a pose that is close to the fruit, to make the next movement easier and more precise to achieve its goal. \nWe set it a few centimeters before the fruit, and we define new function to calculate this pose, from the grasping pose. ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 63,
      "section": "4.4.3. Grasp the fruit"
    }
  },
  {
    "page_content": "from scipy.spatial.transform import Rotation as R\n\ndef get_pregrasping_pose(goal_pose: npt.NDArray[np.float64], target_side : str) -> None:\n    bent_position = np.array([0.38622, 0.22321, -0.27036]) if target_side == 'left' else np.array([0.38622, -0.22321, -0.27036])\n    pregrasp_pose = goal_pose.copy()\n\n    #get the x orientation of the target pose\n    rotation = R.from_matrix(goal_pose[:3,:3]).as_euler('xyz', degrees=False)[0]\n    \n    #pregrasp pose is 8cm before the goal pose\n    pregrasp_pose[0,3] -= 0.08\n    # y value is calculated from the new x value and the effector orientation needed to grasp the object\n    pregrasp_pose[1,3] = pregrasp_pose[0,3] * np.tan(rotation) + bent_position[1]\n\n    return pregrasp_pose\n\nprint(\"Function get_pregrasping_pose defined.\")",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 64,
      "section": "4.4.3. Grasp the fruit"
    }
  },
  {
    "page_content": "We calculate the pregrasp pose for our closest fruit : ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 65,
      "section": "4.4.3. Grasp the fruit"
    }
  },
  {
    "page_content": "pregrasp_pose = get_pregrasping_pose(grasp_pose, target_side)\nprint(f'The pregrasp pose is  : {pregrasp_pose}')",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 66,
      "section": "4.4.3. Grasp the fruit"
    }
  },
  {
    "page_content": "Now, we have our grasp pose and an intermediate pose to reach the object. So we can make Reachy move !\n\nTo do that, we will convert the end effector poses into joints values, using the SDK method *inverse_kinematics* : we will get the 7 joint values of the arm needed to reach this pose. This way, we are gonna be sure that the pose is reachable by the arm, and then, we'll be able to give those commands to the robot. ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 67,
      "section": "4.4.3. Grasp the fruit"
    }
  },
  {
    "page_content": "#we set the controled arm to the left or right arm depending on the target side\narm = reachy.l_arm if target_side == 'left' else reachy.r_arm\n\njoints_to_pregrasp = arm.inverse_kinematics(pregrasp_pose)\njoints_to_grasp = arm.inverse_kinematics(grasp_pose)\n\nprint(f\"Joints to pregrasp : {joints_to_pregrasp}\\n Joints to grasp : {joints_to_grasp}\")",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 68,
      "section": "4.4.3. Grasp the fruit"
    }
  },
  {
    "page_content": "If you have an error on this command, thats means that one of the poses is not reachable by Reachy, so you can move the object on the table and retry by starting back there : \n> Execute the cell below only if you had an error above and you moved the fruits. (*select all the cell and press Ctrl + K + U*)",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 69,
      "section": "4.4.3. Grasp the fruit"
    }
  },
  {
    "page_content": "# oranges, apples, plate, bowl = get_selected_objects(perception, obj_to_left, obj_to_right, left_obj_target, right_obj_target, detection_threshold)\n# closest_object, target_side = get_closest_object(reachy, former_object, oranges, apples, plate, bowl, 0.15)\n# grasp_pose = get_goal_pose(closest_object, target_side)\n# pregrasp_pose = get_pregrasping_pose(grasp_pose, target_side)\n\n# arm = reachy.l_arm if target_side == 'left' else reachy.r_arm\n# joints_to_pregrasp = arm.inverse_kinematics(pregrasp_pose)\n# joints_to_grasp = arm.inverse_kinematics(grasp_pose)\n# print(f\"Joints to pregrasp : {joints_to_pregrasp}\\n Joints to grasp : {joints_to_grasp}\")",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 70,
      "section": "4.4.3. Grasp the fruit"
    }
  },
  {
    "page_content": "Then, we give Reachy the command, thanks to the SDK method *goto()*: \n- first, it will go the pregrasp pose\n- then, to the grasp pose\n\nMake the robot move now : ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 71,
      "section": "4.4.3. Grasp the fruit"
    }
  },
  {
    "page_content": "move_to_pregrasp_id = arm.goto(joints_to_pregrasp, duration = 2)\nmove_to_grasp_id = arm.goto(joints_to_grasp, duration = 2)",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 72,
      "section": "4.4.3. Grasp the fruit"
    }
  },
  {
    "page_content": "Now that Reachy got its gripper around the object, we can close it to grasp the fruit. ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 73,
      "section": "4.4.3. Grasp the fruit"
    }
  },
  {
    "page_content": "arm.gripper.close()",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 74,
      "section": "4.4.3. Grasp the fruit"
    }
  },
  {
    "page_content": "Then, we can raise the arm, using the SDK method translate_by() : ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 75,
      "section": "4.4.3. Grasp the fruit"
    }
  },
  {
    "page_content": "arm.translate_by(0,0,0.1, duration = 1)",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 76,
      "section": "4.4.3. Grasp the fruit"
    }
  },
  {
    "page_content": "Perfect, we made it ! \n\nBut as you can see, Reachy seems a bit sad, keeping its head still. We will add a head movement for Reachy to look at the fruit it wants to catch. \nWe use the method *forward_kinematics()* to get the pose of the effector in Reachy's frame, and we make the head look at its position. \n\nMake the robot move now : ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 77,
      "section": "4.4.3. Grasp the fruit"
    }
  },
  {
    "page_content": "actual_pose = arm.forward_kinematics()\nreachy.head.look_at(actual_pose[0,3], actual_pose[1,3], actual_pose[2,3], 2)",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 78,
      "section": "4.4.3. Grasp the fruit"
    }
  },
  {
    "page_content": "That's much better, don't you think ? \n\nWe are going to define a function for it, to execute it easily, just giving it the ReachySDK and the object we want it to look at as parameters. ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 79,
      "section": "4.4.3. Grasp the fruit"
    }
  },
  {
    "page_content": "def move_head(reachy: ReachySDK, target_obj: Dict) -> None:\n    target_pose = target_obj['pose']\n    reachy.head.look_at(target_pose[0,3], target_pose[1,3], target_pose[2,3], 2)\n\nprint(\"Function move_head defined.\")",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 80,
      "section": "4.4.3. Grasp the fruit"
    }
  },
  {
    "page_content": "So now we have all the building blocks we need to construct a function to link this entire sequence together.\n\nSome adjustements have been made for all the moves to work finely together : \n- on the grasping movement, we use a blocking parameter (*wait=True*), for the gripper to close only when the effector is at the right place\n- we wait until the gripper finished to close before raising the arm\n- we use a blocking parameter also on the last translation for the next moves to wait for the end of all this sequence. ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 81,
      "section": "4.4.3. Grasp the fruit"
    }
  },
  {
    "page_content": "def move_to_grasp(reachy: ReachySDK, obj_to_catch: Dict, target_side: str) -> None:\n    print(\"Move sequence to grasp : started.\")\n\n    move_head(reachy, obj_to_catch)\n\n    grasp_pose = get_goal_pose(obj_to_catch, target_side)\n    pregrasp_pose = get_pregrasping_pose(grasp_pose, target_side)\n    \n    arm = reachy.l_arm if target_side == 'left' else reachy.r_arm\n    joints_to_pregrasp = arm.inverse_kinematics(pregrasp_pose)\n    joints_to_grasp = arm.inverse_kinematics(grasp_pose)\n\n    move_to_pregrasp_id = arm.goto(joints_to_pregrasp)\n    move_to_grasp_id = arm.goto(joints_to_grasp, wait=True)\n        \n    #we wait for the gripper to end closing before raising the arm\n    arm.gripper.close()\n    while arm.gripper.is_moving():\n        time.sleep(0.5)\n    arm.translate_by(x=0, y=0, z=0.1, duration = 1)\n    time.sleep(1)\n\n    print(\"Move sequence to grasp : done.\")\n\nprint(\"Function move_to_grasp defined.\")",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 82,
      "section": "4.4.3. Grasp the fruit"
    }
  },
  {
    "page_content": "To try it, we are gonna first drop the fruit and go back to waiting position.\n\nMake the robot move now : ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 83,
      "section": "4.4.3. Grasp the fruit"
    }
  },
  {
    "page_content": "translation_move = arm.translate_by(0,0,-0.1, duration = 1)\ntime.sleep(1)\narm.gripper.open()\nwhile arm.gripper.is_moving():\n    time.sleep(0.1)\nget_to_waiting_pose(reachy, duration = 3)",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 84,
      "section": "4.4.3. Grasp the fruit"
    }
  },
  {
    "page_content": "Now with only 3 functions, we are going to : \n1. do a new object detection\n2. select the new closest object \n3. make Reachy do the move sequence to grasp it. \n\nMake the robot move now : ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 85,
      "section": "4.4.3. Grasp the fruit"
    }
  },
  {
    "page_content": "oranges, apples, plate, bowl = get_selected_objects(perception, obj_to_left, obj_to_right, left_obj_target, right_obj_target, detection_threshold)\nclosest_object, target_side = get_closest_object(reachy, former_object, oranges, apples, plate, bowl, 0.15)\nmove_to_grasp(reachy, closest_object, target_side)",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 86,
      "section": "4.4.3. Grasp the fruit"
    }
  },
  {
    "page_content": "Well done ! Now let's continue to the drop sequence. ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 87,
      "section": "4.4.3. Grasp the fruit"
    }
  },
  {
    "page_content": "#### 4.4.4. Drop the fruit\n\nWe are going to build a sequence similar to the previous one to drop the grasped fruit in its target object. Hopefully, we defined a lot of functions that will be helpful in this part too !\n\nThe first step is to determine the drop pose. To do that, we are gonna use the centroid of the target object, that we determined thanks to the target_side of the closest object. \n\nAs we want it to drop the fruit on the object, and not catch the object, we are going to virtually increase the z value of the centroid, for the effector to get on top of the object and to avoid any collision. Here, we add 15 cm. \n> Of course, it will depend on the type of object you're using (something rather flat or hollow), you can adjust the value it yourself after testing it. ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 88,
      "section": "4.4.4. Drop the fruit"
    }
  },
  {
    "page_content": "# set the arm and the target object depending on the side of the target\narm = reachy.l_arm if target_side == 'left' else reachy.r_arm\ntarget_obj = plate.copy() if target_side == 'left' else bowl.copy()\n\n#raise virtually the target object by 15cm\ntarget_obj['pose'][2,3] += 0.15 \n\nprint(f\"{target_obj['name']} at {target_obj['pose'][:3,3]}\")",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 89,
      "section": "4.4.4. Drop the fruit"
    }
  },
  {
    "page_content": "We make Reachy look at its new target, with the function *move_head* that we defined ealier. \n\nMake it move now : ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 90,
      "section": "4.4.4. Drop the fruit"
    }
  },
  {
    "page_content": "move_head(reachy, target_obj)",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 91,
      "section": "4.4.4. Drop the fruit"
    }
  },
  {
    "page_content": "Then, we can use the function *get_goal_pose* we defined earlier to get the drop pose as a 4x4 matrix : ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 92,
      "section": "4.4.4. Drop the fruit"
    }
  },
  {
    "page_content": "drop_pose = get_goal_pose(target_obj, target_side)\nprint(f'The drop pose is  : {drop_pose}')",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 93,
      "section": "4.4.4. Drop the fruit"
    }
  },
  {
    "page_content": "We can get the joints value needed to reach this pose using the inverse kinematics, as we saw before. That will also check if that pose is reachable for Reachy.",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 94,
      "section": "4.4.4. Drop the fruit"
    }
  },
  {
    "page_content": "joints_to_drop = arm.inverse_kinematics(drop_pose)\nprint(f\"Joints to drop pose : {joints_to_drop}\")",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 95,
      "section": "4.4.4. Drop the fruit"
    }
  },
  {
    "page_content": "If you have an error on the cell above, that means that your target object is unreachable for Reachy's arm. If so, you can try moving the objet and execute the cell below until you don't have any more error : \n\n> Execute the cell below only if you had an error on the cell above and you did move the target object. (*select all the cell and press Ctrl + K + U*)",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 96,
      "section": "4.4.4. Drop the fruit"
    }
  },
  {
    "page_content": "# oranges, apples, plate, bowl = get_selected_objects(perception, obj_to_left, obj_to_right, left_obj_target, right_obj_target, detection_threshold)\n# target_obj = plate.copy() if target_side == 'left' else bowl.copy()\n# target_obj['pose'][2,3] += 0.15 \n# drop_pose = get_goal_pose(target_obj, target_side)\n# joints_to_drop = arm.inverse_kinematics(drop_pose)",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 97,
      "section": "4.4.4. Drop the fruit"
    }
  },
  {
    "page_content": "Now, you can send the command to the robot and make it move : ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 98,
      "section": "4.4.4. Drop the fruit"
    }
  },
  {
    "page_content": "arm.goto(joints_to_drop)",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 99,
      "section": "4.4.4. Drop the fruit"
    }
  },
  {
    "page_content": "The effector is now on top of the object, but maybe a bit too high : we can translate it a bit closer.\n> In the same way, you can adjust the translation distance to suit your environment.\n\nMake it move now : ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 100,
      "section": "4.4.4. Drop the fruit"
    }
  },
  {
    "page_content": "arm.translate_by(0,0,-0.03, duration = 1)\n",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 101,
      "section": "4.4.4. Drop the fruit"
    }
  },
  {
    "page_content": "Now we can set the object free, by opening the gripper !",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 102,
      "section": "4.4.4. Drop the fruit"
    }
  },
  {
    "page_content": "arm.gripper.open()",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 103,
      "section": "4.4.4. Drop the fruit"
    }
  },
  {
    "page_content": "We raise the arm a little to avoid collisions : ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 104,
      "section": "4.4.4. Drop the fruit"
    }
  },
  {
    "page_content": "arm.translate_by(x=0, y=0, z=0.05, duration = 1)",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 105,
      "section": "4.4.4. Drop the fruit"
    }
  },
  {
    "page_content": "And we get back to the waiting pose :",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 106,
      "section": "4.4.4. Drop the fruit"
    }
  },
  {
    "page_content": "get_to_waiting_pose(reachy)",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 107,
      "section": "4.4.4. Drop the fruit"
    }
  },
  {
    "page_content": "And that's it, well done !\n\nNow we can put it all together in a function. \nSome adjustments have been made for all the moves to work finely together : \n- on the dropping movement, we use a blocking parameter (*wait=True*), for the gripper to open only when the effector is at the right place\n- we wait until the gripper finished to open before raising the arm\n- we use a blocking parameter also on the last translation for the next moves to wait for the end of all this sequence. ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 108,
      "section": "4.4.4. Drop the fruit"
    }
  },
  {
    "page_content": "def move_to_drop(reachy: ReachySDK, target_side: str) -> None:\n    print(\"Move sequence to drop : started.\")\n    \n    arm = reachy.l_arm if target_side == 'left' else reachy.r_arm\n    target_obj = plate.copy() if target_side == 'left' else bowl.copy()\n    \n    target_obj['pose'][2,3] += 0.15\n\n    move_head(reachy, target_obj)\n    drop_pose = get_goal_pose(target_obj, target_side)\n    joints_to_drop = arm.inverse_kinematics(drop_pose)\n    move_to_predrop_id = arm.goto(joints_to_drop)\n    move_to_drop_id = arm.translate_by(0,0,-0.03, duration = 1, wait = True)\n\n    arm.gripper.open()\n    while arm.gripper.is_moving():\n        time.sleep(0.1)\n    arm.translate_by(0,0,0.05, duration = 1, wait = True)\n\n    print(\"Move sequence to drop : done\")\n\nprint(\"Function move_to_drop defined.\")",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 109,
      "section": "4.4.4. Drop the fruit"
    }
  },
  {
    "page_content": "Let's try it ! Place the fruit on the table in front of Reachy. And we'll start from the beginning.\nReachy is going to detect the objects, select the closest one, grasp it and drop it in its target object. \n\nMake it move now : ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 110,
      "section": "4.4.4. Drop the fruit"
    }
  },
  {
    "page_content": "oranges, apples, plate, bowl = get_selected_objects(perception, obj_to_left, obj_to_right, left_obj_target, right_obj_target, detection_threshold)\nclosest_object, target_side = get_closest_object(reachy, former_object, oranges, apples, plate, bowl, 0.15)\nmove_to_grasp(reachy, closest_object, target_side)\nmove_to_drop(reachy, target_side)",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 111,
      "section": "4.4.4. Drop the fruit"
    }
  },
  {
    "page_content": "#### 4.4.5. Get back to the waiting position\n\nWe only have to use our function : ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 112,
      "section": "4.4.5. Get back to the waiting position"
    }
  },
  {
    "page_content": "get_to_waiting_pose(reachy, duration = 3)",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 113,
      "section": "4.4.5. Get back to the waiting position"
    }
  },
  {
    "page_content": "### 4.5. Make it work autonomously\n\nWe have all our functions working. Now we want Reachy to do all of this sequence autonomously, and sort fruits until there are none left. \nFor that, we need to construct a loop and that requires to deal first with the unreachability errors that can happen. ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 114,
      "section": "4.5. Make it work autonomously"
    }
  },
  {
    "page_content": "#### 4.5.1. Manage reachability errors\n\nIf a fruit or a target object is out of reach for Reachy's arm, there will be an error in the code when you call the inverse_kinematics() with the pose matrix of the object. To deal with that, we are going to add a handling of the exceptions.\nWhen an error raises, we want Reachy to notify us that he can't go further. \n\nFor that, we are gonna add a <code>try/except</code> in the loop and define a function for Reachy to make no from head. ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 115,
      "section": "4.5.1. Manage reachability errors"
    }
  },
  {
    "page_content": "def make_no_from_head(reachy : ReachySDK)-> None:\n    print(\"Making a no from the head\")\n    reachy.head.look_at(x=0.5, y=0.15, z=0.15, duration=1)\n    reachy.head.look_at(x=0.5, y=-0.15, z=0.15, duration=1)\n    reachy.head.look_at(x=0.5, y=0.15, z=0.15, duration=1)\n    reachy.head.look_at(x=0.5, y=0, z=-0.25, duration=1, wait=True)\n\nprint(\"Function make_no_from_head defined.\")",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 116,
      "section": "4.5.1. Manage reachability errors"
    }
  },
  {
    "page_content": "You can try it on Reachy : ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 117,
      "section": "4.5.1. Manage reachability errors"
    }
  },
  {
    "page_content": "make_no_from_head(reachy)",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 118,
      "section": "4.5.1. Manage reachability errors"
    }
  },
  {
    "page_content": "When Reachy can't grasp the fruit, it will make no from head and that's it.\n\nBut when he can't reach the target object, it will already have the object in the gripper, so it also needs to put it back on the table. So we are going to define a function to organize the sequence :\n- make no from head\n- lower the effector \n- open the gripper\n- go back to the waiting pose. ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 119,
      "section": "4.5.1. Manage reachability errors"
    }
  },
  {
    "page_content": "def target_object_unreachable(reachy: ReachySDK, target_side: str) -> None:\n    make_no_from_head(reachy)\n    arm = reachy.l_arm if target_side == 'left' else reachy.r_arm\n    arm.translate_by(0,0,-0.1, wait = True)\n    arm.gripper.open()\n    if arm.gripper.is_moving() : \n        time.sleep(0.1)\n    get_to_waiting_pose(reachy)\n\nprint(\"Function target_object_unreachable defined.\")",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 120,
      "section": "4.5.1. Manage reachability errors"
    }
  },
  {
    "page_content": "#### 4.5.2. Make the loop\n\nSo now, we can build the autonomous program !",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 121,
      "section": "4.5.2. Make the loop"
    }
  },
  {
    "page_content": "Let's put all together the different functions we define earlier, to make it work autonomously. \n\nUntil you press the 'interrupt' button, the program will : \n- detect the objects, with the detection threshold you set (*be careful, it waits to detect the two target objects before moving*)\n- select the closest fruit if there is still fruit to grasp\n- move to grasp pose, unless it's not reachable (Reachy will make no from head and go back to the loop beginning)\n- move to drop pose, unless it's not reachable (it will drop back the fruit and make no from head before getting back to its waiting position)\n- get back to its waiting position and do it again\n\nIf there is no fruit left, the program will wait until you interrupt it. \n\n\n**So you can try different things :**\n> - give a fruit to the robot and wait for it to grasp it. \n> - put a fruit far from the robot,\n> - put a fruit in a reachable place but move the target object far from the robot.",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 122,
      "section": "4.5.2. Make the loop"
    }
  },
  {
    "page_content": "closest_object = []\ndetection_threshold = 0.2\n#definition of the target object radius, used for the threshold of object_in_target function\ntarget_object_radius = 0.12\n\ntry:\n    while True:\n        former_object = closest_object\n        oranges, apples, plate, bowl = get_selected_objects(perception, obj_to_left, obj_to_right, left_obj_target, right_obj_target, detection_threshold)\n\n        #get the closest object to its corresponding end-effector \n        closest_object, target_side = get_closest_object(reachy, former_object, oranges, apples, plate, bowl, target_object_radius)\n        print('closest object', closest_object['name'] if closest_object else None)\n        \n        if closest_object : \n            print(f\"target_side : {target_side} for {closest_object['name']}\")\n\n            try : \n                move_to_grasp(reachy, closest_object, target_side)\n\n            except ValueError as e: \n                print(\"Grasping poses not reachable.\")\n                make_no_from_head(reachy)\n                time.sleep(3)\n                continue\n\n\n            try :\n                move_to_drop(reachy, target_side)\n                get_to_waiting_pose(reachy)\n            \n            except ValueError as e: \n                print(\"Dropping poses not reachable.\")\n                target_object_unreachable(reachy,target_side)\n                time.sleep(3)\n        \n        else : \n            print(\"No new object to grasp, waiting for a new one.\")\n            time.sleep(3)\n\nexcept KeyboardInterrupt:\n    pass\n",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 123,
      "section": "4.5.2. Make the loop"
    }
  },
  {
    "page_content": "Once you have finished detecting fruits, you can stop the loop by pressing the \"interrupt\" button and press \"q\" on the annotated image.\n\nThen we are going to make Reachy go step back from the table and going back to a standard pose, to be able to switch off safely.\n\nMake it move now : ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 124,
      "section": "4.5.2. Make the loop"
    }
  },
  {
    "page_content": "reachy.mobile_base.translate_by(-0.2,0)\ntime.sleep(1)\nreachy.goto_posture('default', duration = 2, wait = True)",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 125,
      "section": "4.5.2. Make the loop"
    }
  },
  {
    "page_content": "Then we turn Reachy off smoothly and disconnect from it. ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 126,
      "section": "4.5.2. Make the loop"
    }
  },
  {
    "page_content": "reachy.turn_off()\nreachy.disconnect()\nprint(\"Reachy is disconnected.\")",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_code",
      "cell_number": 127,
      "section": "4.5.2. Make the loop"
    }
  },
  {
    "page_content": "Well done, you did it, you have trained a Reachy greengrocer !",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 128,
      "section": "4.5.2. Make the loop"
    }
  },
  {
    "page_content": "## 5. Final tips\n\nNow, you've learned how to build a sequence on Reachy using the SDK Client and Pollen-Vision : \n- object detection\n- synchronization between Reachy's parts\n- grasping movements\n- combination of perception and movement\n\n\nYou can now use this sequence as a starting point to create other complex behaviors on the robot. Feel free to modify the movements, the duration of the movements, the order of the movements, etc. to get more familiar with the SDK, the robot and to check whether you can make Reachy do what you want it to do ! \n\nHere are some general tips to keep in mind to be the best at implementing complex behaviors on Reachy:\n\n- **Always test behavior** on a fake robot before running it on the real robot! This will help you check if the behavior is doing what you expect it to do and to avoid any potential damage to the robot.\n- If you are working on the real robot, make sure that it has enough space around it to move its arms and head, and **always** have the emergency shutdown nearby. Especially, make sure that the arms will not be blocked by objects such as a table as there are no safety yet preventing the robot for colliding with its environment.\n- Split the behavior you wish to develop into smaller parts and implement them one by one. Once each part is working, you can combine them to create the full sequence. Go slow and test each part before moving on to the next one. \n\n\n## 6. Skip to the next tutorial ! \n\nHere, we've covered just a few of the methods that can be used on Reachy. To discover more ways of controlling the robot, don't hesitate to continue following the tutorials ! \n\n1. Reachy's awakening (with SDK only)\n\n2. Reachy the mime (with SDK only)\n\n**3. Reachy the greengrocer (with SDK & Pollen-Vision)**\n\n\nKeep up and you'll be soon an expert to control Reachy ! ",
    "metadata": {
      "source": "3_Reachy_the_greengrocer.ipynb",
      "type": "notebook_markdown",
      "cell_number": 129,
      "section": "5. Final tips"
    }
  },
  {
    "page_content": "# Tutorial n\u00b01 - SDK : Reachy's awakening\n\nIn this tutorial, we will learn how do a task with Reachy using the SDK client. \n\nHere, we are going to make Reachy do the awake sequence, which is a series of movements that makes it look like it is waking up. It involves moving its head and arms and can be used as a starting point for more complex sequences.\n\nWhat you will learn:\n- How to make it move its head\n- How to make it move its arms\n- How to synchronize head and arms movements\n\n\n## 1. Prerequisites\n\nTo use the SDK client, you first need to install *reachy2-sdk*. If it is not the case, checkout the section below \ud83d\udc47\n<details>\n\n<summary>Install the Python library reachy2-sdk</summary>\n\nIn general, you'd better **work in a virtual environment**. You have 2 different ways to install the sdk : \n- by running the following command:\n\n<code>\npip install reachy2-sdk -e .\n</code>\n\n- from source by following the instructions on the [GitHub repository](https://github.com/pollen-robotics/reachy2-sdk)\n\n</details>\n\nIf you have never used the SDK client before, don't forget to do the [Getting Started notebooks](https://github.com/pollen-robotics/reachy2-sdk/tree/develop/src/examples), that will help you understand all the basics you need to know about it ! And if you want to go further, you can also check the [SDK documentation](https://pollen-robotics.github.io/reachy2-sdk/reachy2_sdk.html). \n\n## 2. Setup\n\n### 2.1. Material\n\nFor this tutorial, you'll only need Reachy and that's it ! \n\nGenerally speaking, before making the real robot move, working on a **virtual Reachy is strongly recommended** to visualize the movements and try to modify them. For that, you can follow the documentation [ajout du lien] and instanciate a ReachySDK with the *IP = 'localhost'*. Once you're sure that all your moves are safe for the robot, you can work on the real Reachy. \n\n### 2.2. Scene\n\n> Your Reachy must be at a sufficient **height** so that its outstretched arms do not touch the mobile base. \n\nPut Reachy in a safe environment with enough place to move around, no one in reachable space and no obstacles (as there are no safety yet preventing the robot from colliding with its environment). And always keep the emergency stop button nearby! \n\n\n\n## 3. Preview\n\nIn this tutorial, we'll build step by step the program that will enable Reachy to look like it's awakening. Here, you can have a look at what Reachy will do at the end :\n\n<p align=\"center\">\n  <img src=\"images/gif_awake.gif\" alt=\"Gif 1\" width=\"30%\">\n</p>\n\nNow that you are all set, let's dig into it ! ",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_markdown",
      "cell_number": 1,
      "section": "Tutorial n\u00b01 - SDK : Reachy's awakening"
    }
  },
  {
    "page_content": "## 4. Let's build it !\n\n### 4.1. Instanciation of the SDK\n\nFirst, we will connect to the robot. To do so, we need to import the SDK client package and to instanciate a ReachySDK.\n\nTwo requirements :\n- Your computer needs to be on the same network as the robot.\n- You need to know your Reachy's IP : to do so, you have two options : \n    - you can check it on the dashboard (*.local_IP_adress_of_your_robot:8000*), section Network. \n    - you can have a look at the small screen on Reachy's back, that will show you one at a time its Ethernet and its Wifi IP. \n\nNow, let's connect to it.",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_markdown",
      "cell_number": 2,
      "section": "4. Let's build it !"
    }
  },
  {
    "page_content": "#import the package\nfrom reachy2_sdk import ReachySDK\n\n#connect to the robot\nreachy = ReachySDK(host=\"localhost\")  # replace 'localhost' with the actual IP address of your Reachy\nprint(\"Reachy is connected :\", reachy.is_connected())",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_code",
      "cell_number": 3,
      "section": "4. Let's build it !"
    }
  },
  {
    "page_content": "If you are getting the message \"Could not connect to Reachy\", make sure that :\n-  Reachy is turned on\n- the reachy2_core.service is running \n- the IP address is correct. \n\n*More info on the debug section of the [sdk documentation](https://pollen-robotics.github.io/reachy2-docs/help/help/recovering/).<br>*",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_markdown",
      "cell_number": 4,
      "section": "4. Let's build it !"
    }
  },
  {
    "page_content": "### 4.2. Set Reachy ready\n\nReachy has its motors off by default. So first, we need to turn them on. ",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_markdown",
      "cell_number": 5,
      "section": "4.2. Set Reachy ready"
    }
  },
  {
    "page_content": "reachy.turn_on()\nprint(f\"Reachy's motors are on : {reachy.is_on()}\")",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_code",
      "cell_number": 6,
      "section": "4.2. Set Reachy ready"
    }
  },
  {
    "page_content": "Then, we are going to place the robot in a neutral position, with the head looking straight ahead and the arms alongside its torso. You can use the *goto_posture* method to do so. ",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_markdown",
      "cell_number": 7,
      "section": "4.2. Set Reachy ready"
    }
  },
  {
    "page_content": "reachy.goto_posture(\"default\")",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_code",
      "cell_number": 8,
      "section": "4.2. Set Reachy ready"
    }
  },
  {
    "page_content": "Is your Reachy looking like this?\n\n<p align=\"center\">\n  <img width=200 src=\"images/default_position.png\" alt=\"Default position of Reachy\" />\n</p>\n\nGreat! Let's start implementing the awake sequence!\n\n*If not, you may need to restart the core of your robot and go back from the beginning*.",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_markdown",
      "cell_number": 9,
      "section": "4.2. Set Reachy ready"
    }
  },
  {
    "page_content": "### 4.3. Build the sequence\n\nThe awake sequence is composed of four parts:\n1. Reachy \"sleeps\", i.e. the arms are swinging alongside its torso and the head is nodding slowly\n2. Reachy looks like it is waking up, lifting its head and moving it from side to side\n3. Reachy lifts its arms, moves them one by one and follows with its head the effector of the moving arm\n4. Reachy nods and gets back to its default position\n\nThe easiest way to implement it (and it is a general rule when implementing complex behavior on the robot) is to break it down into smaller parts and implement them one by one independently. Once each part is working, you can combine them to create the full sequence.\n\nLet's start with the first part: making Reachy \"sleep\".",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_markdown",
      "cell_number": 10,
      "section": "4.3. Build the sequence"
    }
  },
  {
    "page_content": "#### 4.3.1. Asleep part\n\nDuring the asleep sequence, the head of Reachy moves slowly up and down and the arms are outstretched, swinging back and forth. We will implement this sequence first.\n\n##### Head movement\n\nWe will use the *look_at* method to make Reachy's head move up and down. The *look_at* method makes Reachy's head look at a specific point in the robot's frame. We will make Reachy's head look at a point that is above its current position and then below its current position. This will make Reachy's head move up and down.\n\n*If you want more details about the mentioned methods, feel free to check the [documentation](https://pollen-robotics.github.io/reachy2-sdk/reachy2_sdk/parts/head.html) !*\n\nLet's define a function that makes Reachy's head move up and down. We will call this function `asleep_head`.",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_markdown",
      "cell_number": 11,
      "section": "4.3.1. Asleep part"
    }
  },
  {
    "page_content": "def asleep_head(reachy: ReachySDK):\n    for _ in range(2):\n        reachy.head.look_at(x=0.50, y=0.0, z=-0.20, duration=2.0, wait=False)\n        reachy.head.look_at(x=0.50, y=0.0, z=-0.30, duration=2.0, wait=False)\n\nprint(\"Function asleep_head defined.\")",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_code",
      "cell_number": 12,
      "section": "4.3.1. Asleep part"
    }
  },
  {
    "page_content": "Notice that the head will move up and down in a short range. If you want to make the head move in a larger range, just change the value of the z-coordinate in the look_at method.\n\nHere, the parameter *wait* is set to False : we will not wait for the end of the movements in this part of the sequence as we want Reachy to move its head and arms simultaneously later.\n\nNow, you can try it out and check Reachy's head!\n",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_markdown",
      "cell_number": 13,
      "section": "4.3.1. Asleep part"
    }
  },
  {
    "page_content": "asleep_head(reachy)",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_code",
      "cell_number": 14,
      "section": "4.3.1. Asleep part"
    }
  },
  {
    "page_content": "##### Arm movement\n\nThere are different methods available to make Reachy's arms move: *goto* (with the list of joint positions or the goal pose of the effector), *translate_by* and *rotate_by*. Depending on the movement you want to achieve, you will prefer one over the others.\n\nThere, we want Reachy's arms to swing back and forth, standing straight on each side of the torso. \n\nThe simplest way to do it is to place the arms in the default posture, get the current positions of the arms' joints and change the articular value of the shoulder pitch joint, by using the *goto* method. As you learned in the [SDK example](https://github.com/pollen-robotics/reachy2-sdk/blob/develop/src/examples/3_arm_and_gripper.ipynb), the list of joints is as follow : [shoulder pitch, shoulder roll, elbow yaw, elbow pitch, wrist roll, wrist pitch, wrist yaw], so we will change the first value of the list.\n\nLet's define a function that makes Reachy's arms swing back and forth. We will call this function `asleep_arms`.",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_markdown",
      "cell_number": 15,
      "section": "Arm movement"
    }
  },
  {
    "page_content": "def asleep_arms(reachy: ReachySDK):\n    #first, define goal pitch positions for the left arm\n    jpl = reachy.l_arm.get_current_positions() # Considering the arms are in the default position\n    jpl_front = jpl.copy()\n    jpl_front[0] += 6.0\n    jpl_back = jpl.copy()\n    jpl_back[0] -= 6.0\n\n    #then, for the right arm\n    jpr = reachy.r_arm.get_current_positions()\n    jpr_front = jpr.copy()\n    jpr_front[0] += 6.0\n    jpr_back = jpr.copy()\n    jpr_back[0] -= 6.0\n\n    for _ in range(2):\n        # Move left arm backward and right arm forward first and then vice versa\n        reachy.l_arm.goto(jpl_back, duration=2.0, wait=False)\n        reachy.l_arm.goto(jpl_front, duration=2.0, wait=False)\n\n        reachy.r_arm.goto(jpr_front, duration=2.0, wait=False)\n        reachy.r_arm.goto(jpr_back, duration=2.0, wait=False)\n\n    #go back to the default posture for both arms\n    reachy.l_arm.goto_posture(\"default\", wait=False)\n    reachy.r_arm.goto_posture(\"default\", wait=True)\n\nprint(\"Function asleep_arms defined.\")",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_code",
      "cell_number": 16,
      "section": "Arm movement"
    }
  },
  {
    "page_content": "Now, try the function on Reachy and check its arms!",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_markdown",
      "cell_number": 17,
      "section": "Arm movement"
    }
  },
  {
    "page_content": "reachy.goto_posture(\"default\", wait=True)\n\nasleep_arms(reachy)",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_code",
      "cell_number": 18,
      "section": "Arm movement"
    }
  },
  {
    "page_content": "Notice that the arms swing back and forth in a desynchronized way. When the left arm is moving forward, the right arm is moving backward.\n\nFor the range of the arm movement, we chose a range of 12 degrees for the shoulder pitch joint. If you want to change the range of the movement, just change the value 6.0 in the fonction.\n\n##### Combine the head and arm movements\n\nNow that we have implemented both arms and head movements separately and validate them, we can combine them to make the full asleep behavior in a single function `asleep`.",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_markdown",
      "cell_number": 19,
      "section": "Arm movement"
    }
  },
  {
    "page_content": "def asleep(reachy: ReachySDK):\n    asleep_head(reachy)\n    asleep_arms(reachy)\n\nprint(\"Function asleep defined.\")",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_code",
      "cell_number": 20,
      "section": "Arm movement"
    }
  },
  {
    "page_content": "Let's try the function! You should see both sequence for the head and the arms running simultaneously.",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_markdown",
      "cell_number": 21,
      "section": "Arm movement"
    }
  },
  {
    "page_content": "reachy.goto_posture(\"default\", wait=True)\n\nasleep(reachy)",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_code",
      "cell_number": 22,
      "section": "Arm movement"
    }
  },
  {
    "page_content": "> \ud83d\udca1 One important thing to notice is that in order to make the movements of the head and arms synchronized, we need NOT to wait for the end of the movements of the head when we start the movements of the arms (hence the *wait=False* argument in the *look_at* calls). \n> Same goes for the arms, if we wait for the end of the movement, let's say of the left arm before starting the right arm, the movements will be desynchronized.\nCalling the *look_at* and *goto* methods with *wait=False* will load the movements in the goto queue of the SDK which will run them one by one sequentially for each part (left arm, right arm and head).\n\nTry to change some of the *wait* arguments to True, to check how it is impacting the movements !\n\nBecause the duration of the movement of the head and of each arm is the same (4 seconds), we only need to wait for the end of the last movement to be sure that the whole asleep behavior is finished (here we arbitrarily chose to wait for the end of the right arm movement).",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_markdown",
      "cell_number": 23,
      "section": "Arm movement"
    }
  },
  {
    "page_content": "That's it for the asleep part! We can now move on to the next part of the awake sequence: making Reachy looks like it is waking up.",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_markdown",
      "cell_number": 24,
      "section": "Arm movement"
    }
  },
  {
    "page_content": "#### 4.3.2. Waking up part\n\nNow, we want Reachy to lift its head and move it from side to side. \n\nWe are going to define a single function called `head_awake_sequence` to set all the sequence. We can use *look_at* method, with long duration moves to make it look like it's waking up slowly.",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_markdown",
      "cell_number": 25,
      "section": "4.3.2. Waking up part"
    }
  },
  {
    "page_content": "def head_awake_sequence(reachy: ReachySDK):\n    # Lift the head up slowly (it is supposed to be just waking up)\n    reachy.head.look_at(x=0.5, y=0.0, z=0.0, duration=4.0, wait=False)\n\n    # Look left and right\n    reachy.head.look_at(x=0.5, y=-0.2, z=-0.2, duration=2.0, wait=False)\n    reachy.head.look_at(x=0.5, y=0.0, z=-0.05, duration=2.0, wait=False)\n    reachy.head.look_at(x=0.5, y=0.2, z=-0.2, duration=2.0, wait=False)\n\n    # Look straight ahead again\n    reachy.head.look_at(x=0.5, y=0.0, z=0.0, duration=2.0, wait=True)\n\nprint(\"Function head_awake_sequence defined.\")",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_code",
      "cell_number": 26,
      "section": "4.3.2. Waking up part"
    }
  },
  {
    "page_content": "You can now try it on the robot ! \n\n> \ud83d\udca1 Note that we only wait for the last *look_at* to be finished to consider the sequence done. However in this case, since only head movements are composing the awake sequence and are performed one after the other, we could have set wait=True for every *look_at* call, without making any difference. ",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_markdown",
      "cell_number": 27,
      "section": "4.3.2. Waking up part"
    }
  },
  {
    "page_content": "head_awake_sequence(reachy)",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_code",
      "cell_number": 28,
      "section": "4.3.2. Waking up part"
    }
  },
  {
    "page_content": "Perfect ! Now, we can move on to the next step. ",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_markdown",
      "cell_number": 29,
      "section": "4.3.2. Waking up part"
    }
  },
  {
    "page_content": "#### 4.3.3. Arms swinging and end effector tracking\n\nNow, we want Reachy to lift its arms, move them one by one and follow the effector of the moving arm with its head.\n\nThis part is a bit more complex as it involves moving the arms and the head simultaneously and making the robot track itself. \n\nBut don't worry, we are going to break down how we can implement this behavior:\n\n\n1. **Lift the arms and move them one by one**: here we want to place the arms at 90 degree angle and move them one by one forward and backward. \n- To lift the arms at 90 degree angle, we will just call the *goto_posture* method which has it as a predefined posture available.\n- To move the arm, we can use the *translate_by* method : it allows us not to worry about the joints positions needed for this nor getting the appropriate pose matrices for a *goto* call.  We only need to specify the offset from the current pose in the robot's frame that we want to apply to the effector of the arm. </br>\n\n\n2. **Make the head follow the moving arm effector** : as before, we will use the *look_at* method which takes as argument the coordinates x, y, z of a point in the robot's frame. Good news is that we can easily get the coordinates of the effector center in the robot's frame by calling the *forward_kinematics* method of the arm. We will use it to get the coordinates of the moving arm effector and make the head look at this point.\n\nAnd we can define a function `arm_swing_and_effector_tracking` to deal with this entire behavior.\n",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_markdown",
      "cell_number": 30,
      "section": "4.3.3. Arms swinging and end effector tracking"
    }
  },
  {
    "page_content": "def arm_swing_and_effector_tracking(reachy: ReachySDK):\n    # Put the arms at 90 degrees angle and wait for it to be done\n    reachy.goto_posture(\"elbow_90\", wait=True)\n\n    for arm in [reachy.l_arm, reachy.r_arm]:\n        x, y, z = arm.forward_kinematics()[:3, 3]\n        # The first look_at has a longer duration to avoid a sudden head movement\n        reachy.head.look_at(x=x, y=y, z=z, duration=1.5, wait=True)\n\n        arm.translate_by(x=0.2, y=0.0, z=0.1, wait=False) \n        arm.translate_by(x=-0.2, y=0.0, z=-0.1, wait=False)\n        arm.translate_by(x=0.2, y=0.0, z=0.1, wait=False)\n        last_gotoid = arm.translate_by(x=-0.2, y=0.0, z=-0.1, wait=False)\n\n        # while the arm is moving, the head is tracking the effector\n        while not reachy.is_goto_finished(last_gotoid):\n            x, y, z = arm.forward_kinematics()[:3, 3]\n            reachy.head.look_at(x=x, y=y, z=z, duration=0.05, wait = True, interpolation_mode='linear')  # head tracking the effector at 20Hz\n\n        reachy.head.look_at(x=0.5, y=0.0, z=0.0, duration=1.0, wait = True)\n\nprint(\"Function arm_swing_and_effector_tracking defined.\")",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_code",
      "cell_number": 31,
      "section": "4.3.3. Arms swinging and end effector tracking"
    }
  },
  {
    "page_content": "> \ud83d\udca1 Note that we don't wait for the end of the movements of the moving arm in this function as we want the head to follow the effector while the arm is moving. We only wait for the end of the last movement to consider the whole sequence done. While the arm is moving, we call the forward_kinematics method of the arm to get the coordinates of the effector and make the head look at this point.\n\nFor the *look_at* calls of the head, we need to set *wait=True* otherwise a large number of *look_at* calls will be loaded in the goto queue of the sdk and the head will not be able to follow the effector of the moving arm and lots of delay will appear. <br>\nAgain, you can try to change the *wait* argument of the *look_at()*, to see the difference ! ",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_markdown",
      "cell_number": 32,
      "section": "4.3.3. Arms swinging and end effector tracking"
    }
  },
  {
    "page_content": "Now, let's try the function on the robot !",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_markdown",
      "cell_number": 33,
      "section": "4.3.3. Arms swinging and end effector tracking"
    }
  },
  {
    "page_content": "arm_swing_and_effector_tracking(reachy)",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_code",
      "cell_number": 34,
      "section": "4.3.3. Arms swinging and end effector tracking"
    }
  },
  {
    "page_content": "That's working ! Now, we can move on to the last part of our sequence. ",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_markdown",
      "cell_number": 35,
      "section": "4.3.3. Arms swinging and end effector tracking"
    }
  },
  {
    "page_content": "#### 4.3.4. Nodding and getting back to the default posture\n\nTo finish, Reachy nods with its head and gets back to the default posture.\n\nWe will define a function `nod_and_default_posture` for this behavior.\nTo make the nodding part, we will use the *look_at()* method as before. To get back to the default position, we will use the *goto_posture()* method that we also used before in this project.",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_markdown",
      "cell_number": 36,
      "section": "4.3.4. Nodding and getting back to the default posture"
    }
  },
  {
    "page_content": "def nod_and_default_posture(reachy):\n    reachy.head.look_at(0.5, 0.0, -0.2, 1.0)\n    reachy.head.look_at(0.5, 0.0, 0.0, 0.5, wait=True)\n    reachy.l_arm.goto_posture(\"default\")\n    reachy.r_arm.goto_posture(\"default\", wait=True)\n\nprint(\"Function nod_and_default_posture defined.\")",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_code",
      "cell_number": 37,
      "section": "4.3.4. Nodding and getting back to the default posture"
    }
  },
  {
    "page_content": "Now, you can try the function on the robot :",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_markdown",
      "cell_number": 38,
      "section": "4.3.4. Nodding and getting back to the default posture"
    }
  },
  {
    "page_content": "nod_and_default_posture(reachy)",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_code",
      "cell_number": 39,
      "section": "4.3.4. Nodding and getting back to the default posture"
    }
  },
  {
    "page_content": "Perfect ! So now we have all the building blocks we need to construct a function to link this entire sequence together. Let's do it !",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_markdown",
      "cell_number": 40,
      "section": "4.3.4. Nodding and getting back to the default posture"
    }
  },
  {
    "page_content": "### 4.4. Put it all together\n\nNow that we have implemented each behavior separately, it's time to compose them to make the full awake sequence!\n\nThe sequence is just a composition of the different behaviors we implemented and each behavior will be executed sequentially. We will call the functions we defined in the right order to make Reachy do the awake sequence.\n\nWe will gather the different parts of the awake sequence in a single function `awake`.",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_markdown",
      "cell_number": 41,
      "section": "4.4. Put it all together"
    }
  },
  {
    "page_content": "def awake(reachy: ReachySDK):\n    asleep(reachy)\n    head_awake_sequence(reachy)\n    arm_swing_and_effector_tracking(reachy)\n    nod_and_default_posture(reachy)\n\nprint(\"Function awake defined.\")",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_code",
      "cell_number": 42,
      "section": "4.4. Put it all together"
    }
  },
  {
    "page_content": "Call the function to see Reachy doing the whole awake sequence!",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_markdown",
      "cell_number": 43,
      "section": "4.4. Put it all together"
    }
  },
  {
    "page_content": "awake(reachy)",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_code",
      "cell_number": 44,
      "section": "4.4. Put it all together"
    }
  },
  {
    "page_content": "Well done, we did it, Reachy has woken up ! ",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_markdown",
      "cell_number": 45,
      "section": "4.4. Put it all together"
    }
  },
  {
    "page_content": "## 5. Final tips\n\nNow, you've learned how to build a sequence on Reachy using only the SDK Client with: \n- head movements\n- arms movements\n- synchronization between Reachy's parts\n\nYou can now use this sequence as a starting point to create other complex behaviors on the robot. Feel free to modify the movements, the duration of the movements, the order of the movements, etc. to get more familiar with the SDK, the robot and to check whether you can make Reachy do what you want it to do ! \n\nHere are some general tips to keep in mind to be the best at implementing complex behaviors on Reachy:\n\n- **Always test behavior** on a fake robot before running it on the real robot! This will help you check if the behavior is doing what you expect it to do and to avoid any potential damage to the robot.\n- If you are working on the real robot, make sure that it has enough space around it to move its arms and head. Especially, make sure that the arms will not be blocked by objects such as a table as there are no safety yet preventing the robot for colliding with its environment.\n- Split the behavior you wish to develop into smaller parts and implement them one by one. Once each part is working, you can combine them to create the full sequence. Go slow and test each part before moving on to the next one. \n\n\n## 6. Skip to the next tutorial ! \n\nHere, we've covered just a few of the methods that can be used on Reachy. To discover more ways of controlling the robot, don't hesitate to continue following the tutorials ! \n\n**1. Reachy's awakening (with only SDK)**\n\n2. Reachy the mime (with only SDK)\n\n3. Reachy the greengrocer (with SDK & Pollen-Vision)\n\n\nYou'll be soon an expert to control Reachy ! ",
    "metadata": {
      "source": "1_Reachy_awakening.ipynb",
      "type": "notebook_markdown",
      "cell_number": 46,
      "section": "5. Final tips"
    }
  },
  {
    "page_content": "# Tutorial n\u00b02 - SDK : Reachy the mime\n\nIn this tutorial, we will learn how to do a more complex task with Reachy using the SDK client, with both arms and the mobile base, all moving together at the same time !\n\nHere, we are going to transform Reachy into a mime who pulls an invisible rope. \n\nWhat you will learn : \n\n- How to use the mobile base,\n- How to synchronize arms and mobile base movements,\n- How and when to use blocking gotos,\n- How to follow a trajectory with the end effector.\n\n\n\n## 1. Prerequisites\n\nTo use the SDK client, you first need to install *reachy2-sdk*. If it is not the case, checkout the section below \ud83d\udc47\n<details>\n\n<summary>Install the Python library reachy2-sdk</summary>\n\nIn general, you'd better **work in a virtual environment**. You have 2 different ways to install the sdk : \n- by running the following command:\n\n<code>\npip install reachy2-sdk -e .\n</code>\n\n- from source by following the instructions on the [GitHub repository](https://github.com/pollen-robotics/reachy2-sdk)\n\n</details>\n\nIf you have never used the SDK client before, don't forget to do the [Getting Started notebooks](https://github.com/pollen-robotics/reachy2-sdk/tree/develop/src/examples), that will help you understand all the basics you need to know about it ! And if you want to go further, you can also check the [SDK documentation](https://pollen-robotics.github.io/reachy2-sdk/reachy2_sdk.html). \n\n## 2. Setup\n\n### 2.1. Material\n\nFor this tutorial, you'll only need Reachy, and that's it ! \n\nGenerally speaking, before making the real robot move, working on a **virtual Reachy is strongly recommended** to visualize the movements and try to modify them. For that, you can follow the documentation [ajout du lien] and instanciate a ReachySDK with the *IP = 'localhost'*. Once you're sure that all your moves are safe for the robot, you can work on the real Reachy. \nJust know that for this tutorial, you won't be able to test it entirerely on the fake robot, as the mobile base is not set on virtual Reachy. \n\n### 2.2. Scene\n\n> Your Reachy must be at a sufficient **height** so that its outstretched arms do not touch the mobile base. \n\nPut Reachy in a safe environment with enough place to move around, no one in reachable space and no obstacles (as there are no safety yet preventing the robot from colliding with its environment). And always keep the emergency stop button nearby! \n\n\n\n## 3. Preview\n\nIn this tutorial, we'll build step by step the program that will enable Reachy to pull a rope. Here, you can have a look at what Reachy will do at the end :\n\n<p align=\"center\">\n    <img src=\"images/gif_rope.gif\" alt=\"Gif preview\" style=\"width: 30%; display: inline-block;\"/>\n</p>\n\nNow that you are all set, let's dig into it ! ",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_markdown",
      "cell_number": 1,
      "section": "Tutorial n\u00b02 - SDK : Reachy the mime"
    }
  },
  {
    "page_content": "## 4. Let's build it ! \n\n### 4.1. Instanciation of the SDK\n\nFirst, we will connect to the robot. To do so, we need to import the SDK client package and to instanciate a ReachySDK.\n\nTwo requirements :\n- Your computer needs to be on the same network as the robot.\n- You need to know your Reachy's IP : to do so, you have two options : \n    - you can check it on the dashboard (*.local_IP_address_of_your_robot:8000* in a browser), section Network. \n    - you can have a look at the small screen on Reachy's back, that will show you one at a time its Ethernet and its Wifi IP. \n\n> \ud83d\udca1 You'll see that it's really easy to mistakenly run on the real robot before testing in simulation. So you can add a security on your code : if you put the IP of a real robot, the program will ask you a confirmation by pressing \"y\" or \"yes\" before executing anything. \n\nNow, let's connect to it.",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_markdown",
      "cell_number": 2,
      "section": "4. Let's build it !"
    }
  },
  {
    "page_content": "#import the package\nfrom reachy2_sdk import ReachySDK\n\nIP = \"localhost\" # Change the IP to your Reachy's when you are ready to run the code for real\n\n#add the security check\nfake = IP == \"localhost\"\n\nif not fake:\n    res = input(\"Are you sure you want to execute on the real robot ? (y/N)\")\n    if res.lower() not in [\"y\", \"yes\"]:\n        print(\"Aborted.\")\n        exit()\n\n#connect to the robot        \nreachy = ReachySDK(IP)\nreachy.is_connected()",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_code",
      "cell_number": 3,
      "section": "4. Let's build it !"
    }
  },
  {
    "page_content": "If you are getting the message \"Could not connect to Reachy\", make sure that :\n-  Reachy is turned on\n- the reachy2_core.service is running \n- the IP address is correct. \n\n*More info on the debug section of the [sdk documentation](https://pollen-robotics.github.io/reachy2-docs/help/help/recovering/).<br>*",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_markdown",
      "cell_number": 4,
      "section": "4. Let's build it !"
    }
  },
  {
    "page_content": "### 4.2. Set Reachy ready\n\n### 4.2.1. Turn it on\n\nReachy has its motors off by default. So first, we need to turn them on. ",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_markdown",
      "cell_number": 5,
      "section": "4.2. Set Reachy ready"
    }
  },
  {
    "page_content": "reachy.turn_on()\nprint(f\"Reachy's motors are on : {reachy.is_on()}\")",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_code",
      "cell_number": 6,
      "section": "4.2. Set Reachy ready"
    }
  },
  {
    "page_content": "Then, we are going to place the robot in a neutral position, with the head looking straight ahead and the arms alongside its torso. You can use the *goto_posture* method to do so. ",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_markdown",
      "cell_number": 7,
      "section": "4.2. Set Reachy ready"
    }
  },
  {
    "page_content": "reachy.goto_posture('default')",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_code",
      "cell_number": 8,
      "section": "4.2. Set Reachy ready"
    }
  },
  {
    "page_content": "### 4.2.2. The mobile base\n\nNow, let's talk a little about how the mobile base works. \n\nTo make it move, we have two ways : \n- *goto(x, y, theta)* method\n- *translate_by(x,y)* and *rotate_by(theta)* methods\n\n\n#### Goto method  :\n\nWe set the **absolute** target position of the mobile base. Well, it's absolute, relative to the position it was when Reachy was started, or relative to the last position where `reset_odometry()` was called. \n    \nFor example, let's say we just powered the robot. The position of the mobile base is currently `x = 0, y = 0, theta = 0`, `(0, 0, 0)`. \n    \nNow if we call `reachy.mobile_base.goto(0.2, 0, 0)`, after the robot has moved, its position is `(0.2, 0, 0)`. If we call `reachy.mobile_base.goto(0.2, 0, 0)` again, nothing will happen because Reachy is already at this position. \n    \nNow if we call `reachy.mobile_base.reset_odometry()`, it will reset the mobile base's position, meaning its position is now `(0, 0, 0)` again, without moving the mobile base. Its new origin is the current position.\n    \nTo be safe, we always call `reset_odometry()`\u00a0before running our code here, because we may not know how the mobile base moved before. \n\n\n#### Translate_by / rotate_by methods: \n\nYou are safe, the reference position is always Reachy's current position.\n",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_markdown",
      "cell_number": 9,
      "section": "4.2.2. The mobile base"
    }
  },
  {
    "page_content": "Now that Reachy is on, we are going to reset the odometry to be sure (only if you are on the real robot and not the fake one) : ",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_markdown",
      "cell_number": 10,
      "section": "4.2.2. The mobile base"
    }
  },
  {
    "page_content": "if not fake :\n    reachy.mobile_base.reset_odometry()",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_code",
      "cell_number": 11,
      "section": "4.2.2. The mobile base"
    }
  },
  {
    "page_content": "#### 4.2.3. Set the initial position\n\nNow, let's set Reachy in an initial position where its elbows are at 90\u00b0. Conveniently, there is a feature in the SDK to do just that !",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_markdown",
      "cell_number": 12,
      "section": "4.2.3. Set the initial position"
    }
  },
  {
    "page_content": "reachy.goto_posture('elbow_90')",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_code",
      "cell_number": 13,
      "section": "4.2.3. Set the initial position"
    }
  },
  {
    "page_content": "Now, turn its head to the left. ",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_markdown",
      "cell_number": 14,
      "section": "4.2.3. Set the initial position"
    }
  },
  {
    "page_content": "reachy.head.goto([0,10,50])",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_code",
      "cell_number": 15,
      "section": "4.2.3. Set the initial position"
    }
  },
  {
    "page_content": "We save the current poses of the end effectors in *right_start_pose*\u00a0and *left_start_pose* (as 4x4 homogeneous matrices), because we will build the next moves relative to these starting poses. ",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_markdown",
      "cell_number": 16,
      "section": "4.2.3. Set the initial position"
    }
  },
  {
    "page_content": "# Save current effector poses \nright_start_pose = reachy.r_arm.forward_kinematics()\nleft_start_pose = reachy.l_arm.forward_kinematics()\n\nprint(f\"Right arm start pose:\\n {right_start_pose} \\nLeft arm start pose:\\n {left_start_pose}\")",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_code",
      "cell_number": 17,
      "section": "4.2.3. Set the initial position"
    }
  },
  {
    "page_content": "For reference, these are the frames we'll be working with : \n\n<p align=\"center\">\n  <img src=\"images/frame_reachy_default_pose.png\" alt=\"Screenshot 1\" width=\"300\"/>\n  <img src=\"images/frame_reachy_elbow90_pose.png\" alt=\"Screenshot 2\" width=\"300\"/>\n</p>\n\nWe consider the origin of the world to be Reachy's torso. So when we say we translate or rotate in \"absolute\", it means relative to Reachy's torso frame. \n\n> \ud83d\udca1 One important thing to notice is the gripper frames. You can see that they are oriented the same way as the torso when the arms are in the **default** position, meaning along the body. Now look at what happens when we set the arms at 90\u00b0 : forward is `-z`, right is `-y` and up is `+x`. This can be a little counter intuitive when computing poses in the gripper's reference frame, so keep that in mind!",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_markdown",
      "cell_number": 18,
      "section": "4.2.3. Set the initial position"
    }
  },
  {
    "page_content": "Reachy is now ready to start, let's move on to the next step !\n",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_markdown",
      "cell_number": 19,
      "section": "4.2.3. Set the initial position"
    }
  },
  {
    "page_content": "### 4.3. Build the sequence\n\nThe mime sequence is composed of several parts to be synchronized together : \n- extension of one arm and gripper closing\n- flexion of the other arm\n- translation of the mobile base\n\nThe easiest way to build it (and it is a general rule when implementing complex behavior on the robot) is to break it down into smaller parts and implement them one by one independently. Once each part is working, you can combine them to create the full sequence.\n\nLet's start with the first part ! \n\n\n#### 4.3.1. Extension of the arm\n\nLet's compute the extended position for the right end effector. We start from the reference pose and : \n- adjust the position forwards and towards the middle\n- rotate the effector to be perpendicular to the ground and in the robot's *x* axis",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_markdown",
      "cell_number": 20,
      "section": "4.3. Build the sequence"
    }
  },
  {
    "page_content": "from reachy2_sdk.utils.utils import rotate_in_self\n\nright_extend = right_start_pose.copy()\n# Translate by 0.1m in the absolute x direction and by 0.2m in the absolute y direction\nright_extend[:3, 3] += [0.1, 0.2, 0.0] \n# Rotate the end effector by 60\u00b0 around y and -90\u00b0 around z in the gripper's frame\nright_extend = rotate_in_self(right_extend, [0, 60, -90]) \n",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_code",
      "cell_number": 21,
      "section": "4.3. Build the sequence"
    }
  },
  {
    "page_content": "Now we have our goal pose, let's Reachy make the move to reach it and when the move is done, close the gripper :",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_markdown",
      "cell_number": 22,
      "section": "4.3. Build the sequence"
    }
  },
  {
    "page_content": "reachy.r_arm.goto(right_extend, duration=1.5, wait=True)\nreachy.r_arm.gripper.close()",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_code",
      "cell_number": 23,
      "section": "4.3. Build the sequence"
    }
  },
  {
    "page_content": "This is what you should see :\n\n<p align=\"center\">\n    <img src=\"images/pull_rope_1.gif\" alt=\"Gif 1\" width=\"30%\">\n</p>\n\n\n\n\ud83d\udca1 Here, notice that the wait parameter is set to True in our call to *goto*(). \n\n> By default, the goto functions are not blocking, as the default value of *wait* is False, that means they're **not blocking calls** and they will execute asynchronously. They are executed in a separate thread, so they won't block the main thread, meaning the next instruction will be executed at the same time. \n\nHere, and typically in python, the function you call is **blocking**, meaning we have to wait for it to finish executing before executing the next instruction. That allows us to be sure that the gripper will close only when the arm has finished its movement. Without the addition of the *wait=True* parameter, the gripper would have closed during arm movement. You can change the parameter to see for yourself! ",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_markdown",
      "cell_number": 24,
      "section": "4.3. Build the sequence"
    }
  },
  {
    "page_content": "Let's move on to the next part. \n\n#### 4.3.2. Flexion of the arm\n\nNow, we want the right arm to translate along the `x` direction (by 0.2 meters), like if it had grasped a rope and pulled on it. \n\nWe can use the translate_by() method for that. Go ahead and make the arm move : ",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_markdown",
      "cell_number": 25,
      "section": "4.3. Build the sequence"
    }
  },
  {
    "page_content": "reachy.r_arm.translate_by(-0.2, 0, 0, duration=1.5, wait=False)",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_code",
      "cell_number": 26,
      "section": "4.3. Build the sequence"
    }
  },
  {
    "page_content": "Great ! Now, we need to make the mobile base move too. \n\n#### 4.3.3. Translation of the mobile base\n\nWe want the mobile base to move forward by the same amount the right arm has moved to create the illusion that Reachy pulled itself. We can use the goto function to do that. Make the mobile base move now (*only on the real robot*): ",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_markdown",
      "cell_number": 27,
      "section": "4.3. Build the sequence"
    }
  },
  {
    "page_content": "if not fake : \n    reachy.mobile_base.goto(0.2,0,0)",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_code",
      "cell_number": 28,
      "section": "4.3. Build the sequence"
    }
  },
  {
    "page_content": "#### 4.3.4. Make it work together\n\nNow, we need to put all this moves work together at the same time. We want : \n- one arm to go in position to catch the rope\n- the other arm to go backwards to pull the rope\n- the mobile base to go forward\n\nFirst, let's go back to the beginning by moving the mobile base to its origin and by making Reachy get the elbow_90 posture : ",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_markdown",
      "cell_number": 29,
      "section": "4.3.4. Make it work together"
    }
  },
  {
    "page_content": "if not fake : \n    reachy.mobile_base.goto(x=0, y=0, theta=0)\nreachy.goto_posture('elbow_90')\nreachy.head.goto([0,10,50])\nreachy.r_arm.gripper.open()",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_code",
      "cell_number": 30,
      "section": "4.3.4. Make it work together"
    }
  },
  {
    "page_content": "Then, we make Reachy's right arm catch the rope as before : ",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_markdown",
      "cell_number": 31,
      "section": "4.3.4. Make it work together"
    }
  },
  {
    "page_content": "reachy.r_arm.goto(right_extend, duration=1.5, wait=True)\nreachy.r_arm.gripper.close()",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_code",
      "cell_number": 32,
      "section": "4.3.4. Make it work together"
    }
  },
  {
    "page_content": "We build the left extend pose the same way as with the right arm, except that the translation on the y axis needs to be negative this time, to be close to the middle. ",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_markdown",
      "cell_number": 33,
      "section": "4.3.4. Make it work together"
    }
  },
  {
    "page_content": "left_extend = left_start_pose.copy()\nleft_extend[:3, 3] += [0.1, -0.2, 0.0]\nleft_extend = rotate_in_self(left_extend, [0, 60, 90])",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_code",
      "cell_number": 34,
      "section": "4.3.4. Make it work together"
    }
  },
  {
    "page_content": "Then we make all the other moves work together : \n\n> For now, we need to run the translation of the mobile base in a thread to be able to execute command on other Reachy's part in the same time, as the mobile base gotos are blocking, but that will change soon (so stay tuned!). ",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_markdown",
      "cell_number": 35,
      "section": "4.3.4. Make it work together"
    }
  },
  {
    "page_content": "import _thread\nimport time\n\n## launch simultaneously all the moves \n#make the left arm go catch the rope\ngoto_id = reachy.l_arm.goto(left_extend, duration=2.0, wait=False) # Non-blocking\n\n#make the right arm pull the rope\nreachy.r_arm.translate_by(-0.2, 0, 0, duration=1.5, wait=False) #\u00a0Non-blocking\n\n#move the mobile base forward\nif not fake: \n    _thread.start_new_thread(reachy.mobile_base.goto, (0.2, 0, 0))\n\n\n## Wait for the end of the left arm move to close the gripper\nif goto_id.id != 0:\n    while not reachy.is_goto_finished(goto_id):\n        time.sleep(0.1)\nreachy.l_arm.gripper.close()\n\ntime.sleep(2)\n",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_code",
      "cell_number": 36,
      "section": "4.3.4. Make it work together"
    }
  },
  {
    "page_content": "Let's take a moment to understand what's going on here.\n\n`reachy.arm.goto()` returns a `goto id`. This `goto id`\u00a0allows us to ask the SDK if this move is finished using `reachy.is_goto_finished(goto_id)`. \n\nHere, we want to move both arms at the same time. But their durations are not the same (2.0s and 1.5s). We run all the moves in non blocking mode (left arm, right arm and mobile base), but then we want to wait for the first move to end before continuing. To do that, we can block the execution until `reachy.is_goto_finished(goto_id)` is True. \n\nThat's what we do here with the `while not reachy.is_goto_finished(goto_id):` instruction.\n\nThe full move now looks like this : \n\n<p align=\"center\">\n    <img src=\"images/pull_rope_2.gif\" alt=\"Gif 2\" width=\"30%\">\n</p>",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_markdown",
      "cell_number": 37,
      "section": "4.3.4. Make it work together"
    }
  },
  {
    "page_content": "#### 4.3.5. Follow a trajectory\n\nNow, we want :\n- the left arm to translate along `x`the same way the right arm just did\n- the right arm to go back into position to be ready to pull again\n- the mobile base to move forward again\n\nHere is where things get interesting. If we use `arm.translate_by()` or `arm.goto()`, we have no guarantee that the arms won't collide together performing their respective moves (spoiler alert : they will). \n\nSo we want the right arm to go back the extended position by following a trajectory that will avoid the left arm.\n\nTo do that, we can use the feature that interpolates trajectory tracking: `send_cartesian_interpolation()` \nBy default, it's a linear interpolation, but if you set the parameter 'arc_direction', you can follow a semi-circular trajectory until the target position. \nWith the *arc_direction* parameter set to \"right\", the semi circle trajectory will be in the `(x, y)` plane, going to the right.\n\n> \ud83d\udca1 Note that this function is **blocking**. ",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_markdown",
      "cell_number": 38,
      "section": "4.3.5. Follow a trajectory"
    }
  },
  {
    "page_content": "#open the gripper\nreachy.r_arm.gripper.open()\n\n#move the mobile base forward\nif not fake:\n    reachy.mobile_base.reset_odometry()\n    _thread.start_new_thread(reachy.mobile_base.goto, (0.2, 0, 0))\n\n#move the left arm to pull the rope\nreachy.l_arm.translate_by(-0.2, 0, 0, duration=1.5, wait=False)\n\n#move the right arm to catch the rope with a semi-circular trajectory\nreachy.r_arm.send_cartesian_interpolation(\n    right_extend, duration=1.3, arc_direction=\"right\", precision_distance_xyz=0.1\n)\n\n#close the gripper\nreachy.r_arm.gripper.close()",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_code",
      "cell_number": 39,
      "section": "4.3.5. Follow a trajectory"
    }
  },
  {
    "page_content": "Look at the trajectory of the right arm !\n\n<p align=\"center\">\n    <img src=\"images/pull_rope_3.gif\" alt=\"Gif 3\" width=\"30%\">\n</p>",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_markdown",
      "cell_number": 40,
      "section": "4.3.5. Follow a trajectory"
    }
  },
  {
    "page_content": "Now we just have to do the same with the left arm !\nSo at the same time, we : \n- open the left gripper\n- move forward the mobile base\n- pull the invisible rope with the right arm \n- move forward the left arm, using a semi-circular trajectory to the left this time\n\nThen we close the left gripper. \n\nYou can do it on your robot : ",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_markdown",
      "cell_number": 41,
      "section": "4.3.5. Follow a trajectory"
    }
  },
  {
    "page_content": "#open the left gripper\nreachy.l_arm.gripper.open()\n\n#move the mobile base forward\nif not fake:\n    reachy.mobile_base.reset_odometry()\n    _thread.start_new_thread(reachy.mobile_base.goto, (0.2, 0, 0))\n\n#move the right arm to pull the rope\nreachy.r_arm.translate_by(-0.2, 0, 0, duration=1.5, wait=False)\n\n#move the left arm to catch the rope with a semi-circular trajectory\nreachy.l_arm.send_cartesian_interpolation(\n    left_extend, duration=1.3, arc_direction=\"left\", precision_distance_xyz=0.2\n)\n\n#close the left gripper\nreachy.l_arm.gripper.close()",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_code",
      "cell_number": 42,
      "section": "4.3.5. Follow a trajectory"
    }
  },
  {
    "page_content": "Well done ! You did it ! Now that you understood how to do it, you can continue to move Reachy forward, or make a U-turn via the mobile base (*reachy.mobile_base.rotate_by(180)*), or do the same sequence but in reverse! \n\nDon't hesitate to try out and practice all Reachy's features ! \n\nOnce you've finished, you can return Reachy to its initial position and log off. ",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_markdown",
      "cell_number": 43,
      "section": "4.3.5. Follow a trajectory"
    }
  },
  {
    "page_content": "reachy.goto_posture('elbow_90', duration = 3, wait = True)\nreachy.turn_off_smoothly()\nreachy.disconnect()",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_code",
      "cell_number": 44,
      "section": "4.3.5. Follow a trajectory"
    }
  },
  {
    "page_content": "## 5. Final tips\n\nNow, you've learned how to build a sequence on Reachy using only the SDK Client with: \n- arms movements\n- mobile base movements\n- trajectory following\n- synchronization between Reachy's parts\n\nYou can now use this sequence as a starting point to create other complex behaviors on the robot. Feel free to modify the movements, the duration of the movements, the order of the movements, etc. to get more familiar with the SDK, the robot and to check whether you can make Reachy do what you want it to do ! \n\nHere are some general tips to keep in mind to be the best at implementing complex behaviors on Reachy:\n\n- **Always test behavior** on a fake robot before running it on the real robot! This will help you check if the behavior is doing what you expect it to do and to avoid any potential damage to the robot.\n- If you are working on the real robot, make sure that it has enough space around it to move its arms and head, and **always** have the emergency shutdown nearby. Especially, make sure that the arms will not be blocked by objects such as a table as there are no safety yet preventing the robot for colliding with its environment.\n- Split the behavior you wish to develop into smaller parts and implement them one by one. Once each part is working, you can combine them to create the full sequence. Go slow and test each part before moving on to the next one. \n\n\n## 6. Skip to the next tutorial ! \n\nHere, we've covered just a few of the methods that can be used on Reachy. To discover more ways of controlling the robot, don't hesitate to continue following the tutorials ! \n\n1. Reachy's awakening (with SDK only)\n\n**2. Reachy the mime (with SDK only)**\n\n3. Reachy the greengrocer (with SDK & Pollen-Vision)\n\n\nKeep up and you'll be soon an expert to control Reachy ! ",
    "metadata": {
      "source": "2_Reachy_the_mime.ipynb",
      "type": "notebook_markdown",
      "cell_number": 45,
      "section": "5. Final tips"
    }
  }
]