"source": [
    "### 4.2. Instanciation of Pollen-Vision \n",
    "\n",
    "Then, we will use Pollen-Vision repository to be able to detect objects from the camera view thanks to AI and extract informations about them, for Reachy to adapt its actions to its environment.\n",
    "\n",
    "More specifically, we'll be using the *Perception* module, which combines object detection using a [YOLO model](https://docs.ultralytics.com/models/yolo-world/), image segmentation and conversion of the extracted informations from pixels into real-world coordinates. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1. How does it work\n",
    "\n",
    "We set the name of the objects we're interested in, and the model detects those objects with a prediction confidence score (from 0, the lowest, to 1, the highest). And from those detections, we get different informations about the objects such as the coordinates of the bounding box, the centroid, and the depth. As this is done on Reachy's camera view, we can track the objects dynamically and minimize the impact of artifacts by retaining only the objects that are always detected from one frame to the next. \n",
    "\n",