"\n",
    "To test those, you can use directly two submodules used in Perception to see if your objects are correctly detected on your image :\n",
    "-  the YOLO model (*the one that detects objects*) \n",
    "-  the Annotator (*the one that allows the visualization*).\n",
    "\n",
    "Be creative, try even non-instinctive words, with more or less precise adjectives, and test everything you can ! \n",
    "\n",
    "In this tutorial, we decide to define two objects in which to place the fruit: a right-hand side and a left-hand side. And we'll define the type of fruit to be placed on the right and the one on the left. Here, we have a plate on the left and a bowl on the right, and the fruits to be placed are oranges and apples.\n",
    "\n",
    "We are going to try classic labels for the detection, in the candidate_labels, but you'll see that we need to adjust those because the detection won't be good enough. \n",
    "\n",
    "*Don't worry if the instanciation of the YoloWorldWrapper is taking a while, it can take about 1-1.5min to create it the first time. And you can have a warning message, with no impact on the rest of the tutorial*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pollen_vision.vision_models.object_detection import YoloWorldWrapper\n",
    "from pollen_vision.utils import get_bboxes\n",
    "from pollen_vision.utils import Annotator\n",
    "\n",